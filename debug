1. agentic_batch_orchestrator.py:

import time
import os
import requests
from smart_file_loader import SmartFileLoader
from batch_controller import run_batch_controller
from progress_manager import ProgressManager
from error_handler import ErrorHandler
from notification_manager import NotificationManager

DATA_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "data")

EMAIL_CONFIG = {
    # ...isi sesuai kebutuhan produksi...
}

# --- PATCH: Endpoint untuk batch sync ---
BATCH_SIZE = 15000  # atau sesuai kebutuhan
BACKEND_URL = "http://127.0.0.1:8000/all_data_merge"
SYNC_URL = "http://127.0.0.1:8000/force_sync_progress"

def scan_and_report():
    loader = SmartFileLoader(DATA_DIR)
    try:
        tabular = loader.load_all_csv_json_tables()
    except Exception as e:
        print(f"[HYBRID-FALLBACK][ERROR] Failed to load tabular tables: {e}")
        tabular = {}
    try:
        smart = loader.smart_load_all_tables()
    except Exception as e:
        print(f"[HYBRID-FALLBACK][ERROR] Failed to load smart tables: {e}")
        smart = {}
    print("[ORCHESTRATOR] Tabular files loaded:", list(tabular.keys()))
    print("[ORCHESTRATOR] Smart files loaded:", list(smart.keys()))
    return tabular, smart

def process_batch(pm, eh, nm):
    try:
        print("[ORCHESTRATOR] Running batch controller...")
        try:
            run_batch_controller()
        except Exception as e:
            print(f"[HYBRID-FALLBACK][ERROR] run_batch_controller failed: {e}")
            eh.log_error(e, context="process_batch", notify_callback=nm.notify)
        try:
            progress = pm.get_all_progress()
        except Exception as e:
            print(f"[HYBRID-FALLBACK][ERROR] get_all_progress failed in process_batch: {e}")
            eh.log_error(e, context="process_batch", notify_callback=nm.notify)
            progress = {}
        print("[ORCHESTRATOR] Progress:", progress)
        return progress
    except Exception as e:
        eh.log_error(e, context="process_batch", notify_callback=nm.notify)
        return {}

def all_files_finished(progress, loader):
    try:
        all_tables = list(loader.load_all_csv_json_tables().keys()) + list(loader.smart_load_all_tables().keys())
    except Exception as e:
        print(f"[HYBRID-FALLBACK][ERROR] all_files_finished failed to load tables: {e}")
        all_tables = []
    finished = True
    for fname in all_tables:
        entry = progress.get(fname, {})
        processed = entry.get("processed", 0) if isinstance(entry, dict) else entry
        total = entry.get("total", None)
        print(f"[ORCHESTRATOR][CHECK] {fname}: processed={processed}, total={total}")
        if total is None:
            print(f"[ORCHESTRATOR][WARNING] Total tidak ditemukan di progress untuk file: {fname}")
            continue
        if processed < total:
            print(f"[ORCHESTRATOR][INFO] File {fname} belum selesai: {processed}/{total}")
            finished = False
    if finished:
        print("[ORCHESTRATOR][STATUS] Semua file selesai diproses (all_files_finished=True)")
    else:
        print("[ORCHESTRATOR][STATUS] Masih ada file yang belum selesai (all_files_finished=False)")
    return finished

def process_batch_http(offset, limit):
    params = {"limit": limit, "offset": offset}
    resp = requests.get(BACKEND_URL, params=params)
    data = resp.json()
    requests.post(SYNC_URL)
    print(f"[Batch] offset={offset} limit={limit} got={len(data)} rows")
    return data

def main_loop():
    loader = SmartFileLoader(DATA_DIR)
    pm = ProgressManager(DATA_DIR)
    eh = ErrorHandler()
    nm = NotificationManager(email_config=EMAIL_CONFIG)

    # PATCH: Hybrid mode, bisa pilih mode HTTP batch atau local batch_controller
    # Jika ingin menggunakan backend HTTP batching:
    offset = 0
    while True:
        data = process_batch_http(offset, BATCH_SIZE)
        if not data:
            break
        offset += BATCH_SIZE
        time.sleep(1)  # beri delay agar backend selesai update progress

    # --- Jika ingin mode agentic batch lokal, gunakan blok berikut ini ---
    # while True:
    #     print("[ORCHESTRATOR][STEP] Sync progress with files...")
    #     try:
    #         pm.sync_progress_with_files()
    #     except Exception as e:
    #         print(f"[HYBRID-FALLBACK][ERROR] sync_progress_with_files failed in main_loop: {e}")
    #         eh.log_error(e, context="main_loop", notify_callback=nm.notify)
    #     scan_and_report()
    #     progress = process_batch(pm, eh, nm)
    #     if all_files_finished(progress, loader):
    #         print("[ORCHESTRATOR] All files finished processing!")
    #         try:
    #             nm.notify("All files finished processing!", level="info", context="orchestrator")
    #         except Exception as e:
    #             print(f"[HYBRID-FALLBACK][ERROR] notify failed in main_loop: {e}")
    #         break
    #     print("[ORCHESTRATOR][STEP] Sleeping for 5 seconds before next iteration")
    #     time.sleep(5)

print("=== Agentic Orchestrator: Script masuk ===")

if __name__ == "__main__":
    print("=== Agentic Orchestrator: Mulai main_loop ===")
    try:
        main_loop()
    except Exception as e:
        print(f"[HYBRID-FALLBACK][ERROR] main_loop failed: {e}")
    print("=== Agentic Orchestrator: Selesai main_loop ===")

2. batch_controller.py:

import os
import json
import hashlib
import time
from typing import List, Dict, Tuple

import pandas as pd
from progress_manager import ProgressManager
from batch_agent_experta import get_batch_plan  # Integrasi experta
from row_estimator import estimate_csv_rows  # Integrasi estimator cepat

# --- CONFIGURABLE LIMITS ---
TOTAL_BATCH_LIMIT = 15000      # Total quota per global batch
PER_FILE_MAX = 15000           # Max per file per batch
MIN_BATCH_SIZE = 100
DEFAULT_BATCH_SIZE = 15000
CONSECUTIVE_SUCCESS_TO_INCREASE = 3  # Naikkan batch jika sukses berturut-turut

DATA_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "data")
pm = ProgressManager(DATA_DIR)

def calc_sha256_from_file(path):
    try:
        hash_sha256 = hashlib.sha256()
        with open(path, "rb") as f:
            for chunk in iter(lambda: f.read(8192), b""):
                hash_sha256.update(chunk)
        return hash_sha256.hexdigest()
    except Exception as e:
        print(f"[HYBRID-FALLBACK][ERROR] calc_sha256_from_file failed: {e}")
        return ""

def list_data_files(data_dir: str) -> List[str]:
    print(f"[DEBUG] list_data_files: reading from {data_dir}")
    try:
        files = []
        for f in os.listdir(data_dir):
            if f.endswith(".csv") and "progress" not in f and "meta" not in f:
                files.append(f)
        print(f"[DEBUG] list_data_files: files={files}")
        return files
    except Exception as e:
        print(f"[HYBRID-FALLBACK][ERROR] list_data_files failed: {e}")
        return []

def get_file_info(data_dir: str) -> List[Dict]:
    print(f"[DEBUG] get_file_info: collecting file info from {data_dir}")
    files = list_data_files(data_dir)
    info_list = []
    try:
        progress = pm.get_all_progress()  # Untuk cache
    except Exception as e:
        print(f"[HYBRID-FALLBACK][ERROR] get_all_progress failed: {e}")
        progress = {}
    for fname in files:
        fpath = os.path.join(data_dir, fname)
        try:
            size_bytes = os.path.getsize(fpath)
        except Exception as e:
            print(f"[HYBRID-FALLBACK][ERROR] get_file_info os.path.getsize failed for {fname}: {e}")
            size_bytes = 0
        sha256 = calc_sha256_from_file(fpath)
        try:
            modified_time = str(os.path.getmtime(fpath))
        except Exception as e:
            print(f"[HYBRID-FALLBACK][ERROR] get_file_info os.path.getmtime failed for {fname}: {e}")
            modified_time = ""
        # PATCH: total_items SELALU dari meta file (via progress_manager)
        progress_entry = progress.get(fname, {})
        total_items = progress_entry.get("total", 0)
        is_estimated = progress_entry.get("is_estimated", True)
        info_list.append({
            "file": fname,
            "size_bytes": size_bytes,
            "total_items": total_items,
            "sha256": sha256,
            "modified_time": modified_time
        })
        print(f"[DEBUG] File Info: {fname}, size: {size_bytes}, total: {total_items}, sha256: {sha256}, modified: {modified_time}")
    print(f"[DEBUG] get_file_info: info_list={info_list}")
    return info_list

def build_experta_file_status(file_info, progress):
    print(f"[DEBUG] build_experta_file_status called")
    status_list = []
    for info in file_info:
        fname = info["file"]
        entry = progress.get(fname, {})
        processed = entry.get("processed", 0) if isinstance(entry, dict) else 0
        status_list.append({
            "name": fname,
            "size": info["total_items"],
            "total": info["total_items"],
            "processed": processed
        })
        print(f"[DEBUG] Experta Status: name={fname}, size={info['total_items']}, total={info['total_items']}, processed={processed}")
    print(f"[DEBUG] build_experta_file_status: status_list={status_list}")
    return status_list

def experta_batch_distributor(file_info, progress, batch_limit=TOTAL_BATCH_LIMIT):
    print(f"[DEBUG] experta_batch_distributor called")
    file_status_list = build_experta_file_status(file_info, progress)
    print(f"[DEBUG] Calling get_batch_plan with file_status_list={file_status_list}, batch_limit={batch_limit}")
    try:
        batch_plan = get_batch_plan(file_status_list, batch_limit=batch_limit)
    except Exception as e:
        print(f"[HYBRID-FALLBACK][ERROR] get_batch_plan failed: {e}")
        # Fallback: allocate nothing
        batch_plan = []
    print(f"[DEBUG] Received batch_plan={batch_plan}")
    allocations = []
    for plan in batch_plan:
        fname = plan.get("file")
        batch_size = plan.get("batch_size")
        if batch_size == 'all':
            entry = next((item for item in file_status_list if item["name"] == fname), None)
            alloc = entry["total"] - entry["processed"] if entry else 0
        else:
            alloc = batch_size
        allocations.append((fname, alloc))
        print(f"[DEBUG] Experta batch plan: {fname}, alloc={alloc}")
    all_names = [info['file'] for info in file_info]
    planned_names = [x[0] for x in allocations]
    for name in all_names:
        if name not in planned_names:
            allocations.append((name, 0))
            print(f"[DEBUG] Experta: {name} not planned, alloc=0")
    print(f"[DEBUG] experta_batch_distributor: allocations={allocations}")
    return allocations

def simulate_batch_process(file_name, start_idx, end_idx):
    print(f"[DEBUG] simulate_batch_process called: {file_name} idx {start_idx}-{end_idx}")
    if "error" in file_name and (end_idx - start_idx) > 1000:
        print(f"[DEBUG] simulate_batch_process: simulated error (timeout) for {file_name}")
        return False, "timeout"
    return True, None

def process_file_batch(file_name, start_idx, end_idx, batch_size, progress_entry):
    print(f"[BATCH] Proses {file_name} idx {start_idx}-{end_idx}, batch_size={batch_size}")
    try:
        fpath = os.path.join(DATA_DIR, file_name)
        # PATCH: total_items SELALU dari progress_manager/meta file, tidak pernah scan file!
        total_items = progress_entry.get("total", 0)
        success, error_type = simulate_batch_process(file_name, start_idx, end_idx)
        if success:
            consecutive_success_count = progress_entry.get("consecutive_success_count", 0) + 1
            pm.update_progress(
                file_name,
                processed=end_idx,
                last_batch=progress_entry.get("last_batch", 0)+1,
                last_batch_size=batch_size,
                retry_count=0,
                last_error_type=None,
                consecutive_success_count=consecutive_success_count
            )
            print(f"[PROGRESS] {file_name}: processed={end_idx}, total={total_items}")
            return True, batch_size
        else:
            print(f"[ERROR] Batch {file_name} idx {start_idx}-{end_idx} FAILED: {error_type}")
            pm.update_progress(
                file_name,
                processed=progress_entry.get("processed", 0),
                last_batch=progress_entry.get("last_batch", 0),
                last_batch_size=batch_size,
                retry_count=1,
                last_error_type=error_type,
                consecutive_success_count=0
            )
            print(f"[PROGRESS] {file_name}: processed={progress_entry.get('processed', 0)}, total={total_items}, last_error={error_type}")
            return False, batch_size
    except Exception as e:
        print(f"[HYBRID-FALLBACK][EXCEPTION] {file_name} idx {start_idx}-{end_idx} exception: {e}")
        try:
            pm.update_progress(
                file_name,
                processed=progress_entry.get("processed", 0),
                last_batch=progress_entry.get("last_batch", 0),
                last_batch_size=batch_size,
                retry_count=1,
                last_error_type="exception",
                consecutive_success_count=0
            )
            print(f"[PROGRESS] {file_name}: processed={progress_entry.get('processed', 0)}, total={progress_entry.get('total', 'unknown')}, last_error=exception")
        except Exception as e2:
            print(f"[HYBRID-FALLBACK][ERROR] Failed to update progress after exception: {e2}")
        return False, batch_size

def run_batch_controller():
    # PATCH: Locking
    pm = ProgressManager(DATA_DIR)
    with pm.lock:
        print("[BatchController] Running batch controller with locking")
        print("[DEBUG] run_batch_controller: mulai sync_progress_with_files()")
        try:
            pm.sync_progress_with_files()
        except Exception as e:
            print(f"[HYBRID-FALLBACK][ERROR] sync_progress_with_files failed: {e}")
        print("[DEBUG] run_batch_controller: selesai sync_progress_with_files()")
        file_info = get_file_info(DATA_DIR)
        print(f"[DEBUG] run_batch_controller: file_info={file_info}")
        try:
            progress = pm.get_all_progress()
        except Exception as e:
            print(f"[HYBRID-FALLBACK][ERROR] get_all_progress failed in run_batch_controller: {e}")
            progress = {}
        print(f"[DEBUG] run_batch_controller: progress={progress}")
        allocations = experta_batch_distributor(file_info, progress)
        print("Batch allocation this round (experta):")
        for fname, alloc in allocations:
            print(f"  {fname}: {alloc}")
        for fname, alloc in allocations:
            print(f"[DEBUG] Looping allocation: {fname}, alloc={alloc}")
            if alloc <= 0:
                continue
            entry = progress.get(fname, {})
            print(f"[DEBUG] Entry {fname}: {entry}")
            processed = entry.get("processed", 0)
            total = entry.get("total", 0)
            batch_size = entry.get("last_batch_size", DEFAULT_BATCH_SIZE)
            start_idx = processed
            end_idx = min(processed + alloc, total)
            print(f"[DEBUG] Akan proses {fname}: {start_idx}-{end_idx} (batch_size={batch_size})")
            ok, batch_size_used = process_file_batch(fname, start_idx, end_idx, batch_size, entry)
            try:
                entry = pm.get_file_progress(fname)
                print(f"[DEBUG] Setelah process_file_batch {fname}: {entry}")
                if ok and entry.get("consecutive_success_count", 0) >= CONSECUTIVE_SUCCESS_TO_INCREASE:
                    new_size = min(batch_size_used * 2, PER_FILE_MAX)
                    print(f"[DEBUG] Doubling batch_size for {fname} to {new_size}")
                    pm.update_progress(fname, processed=entry.get("processed", 0),
                                       last_batch=entry.get("last_batch", 0),
                                       last_batch_size=new_size, retry_count=0, last_error_type=None,
                                       consecutive_success_count=0)
            except Exception as e2:
                print(f"[HYBRID-FALLBACK][ERROR] Error in post-batch update for {fname}: {e2}")

if __name__ == "__main__":
    print("[DEBUG] __main__ run_batch_controller")
    try:
        run_batch_controller()
    except Exception as e:
        print(f"[HYBRID-FALLBACK][ERROR] run_batch_controller failed: {e}")

3. progress_manager.py:

import os
import json
from filelock import FileLock

class ProgressManager:
    """
    Manajemen file_progress.json: baca, tulis, update, recovery, dan query status file.
    Process-safe (menggunakan filelock) untuk multi-batch/worker paralel.
    Untuk field total record, progress manager SELALU membaca data dari csvjson_gdrive_meta.json (dinamis, tanpa perhitungan ulang).
    Mendukung skema hybrid: jika proses data meta gagal/error, otomatis fallback ke baca data file statis (CSV/JSON).
    Sinkronisasi dengan hasil scan_data_folder dari smart_file_scanner.py.
    """
    def __init__(self, data_dir=None, progress_file=None, meta_file=None):
        if data_dir is None:
            data_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "data")
        if progress_file is None:
            progress_file = os.path.join(data_dir, "file_progress.json")
        if meta_file is None:
            meta_file = os.path.join(data_dir, "csvjson_gdrive_meta.json")
        self.data_dir = data_dir
        self.progress_file = progress_file
        self.meta_file = meta_file
        self.lock = FileLock(self.progress_file + ".lock")
        self._cache = None  # Optional: cache progres di RAM
        print(f"[progress_manager][DEBUG] ProgressManager initialized with data_dir={self.data_dir}, progress_file={self.progress_file}, meta_file={self.meta_file}")

    def load_progress(self):
        """Baca progres dari file (process-safe)."""
        with self.lock:
            if not os.path.exists(self.progress_file):
                print(f"[progress_manager][DEBUG] Progress file not found: {self.progress_file}")
                return {}
            try:
                with open(self.progress_file, "r", encoding="utf-8") as f:
                    data = json.load(f)
                self._cache = data
                print(f"[progress_manager][DEBUG] Progress loaded: {data}")
                return data
            except Exception as e:
                print(f"[progress_manager] Failed to load progress: {e}")
                return {}

    def save_progress(self, progress):
        """Tulis progres ke file (process-safe)."""
        with self.lock:
            try:
                with open(self.progress_file, "w", encoding="utf-8") as f:
                    json.dump(progress, f, indent=2)
                self._cache = progress
                print(f"[progress_manager][DEBUG] Progress saved: {progress}")
            except Exception as e:
                print(f"[progress_manager] Failed to save progress: {e}")

    def update_progress(self, file_name, processed, sha256=None, modified_time=None, last_batch=None, total=None,
                        retry_count=None, last_batch_size=None, last_error_type=None, consecutive_success_count=None, is_estimated=None):
        """
        Update progres untuk satu file. Reset jika file berubah (hash/modif).
        Field 'total' diabaikan di sini, karena akan selalu diambil dari meta file.
        """
        with self.lock:
            print(f"[progress_manager][DEBUG] update_progress called for: {file_name}")
            progress = self.load_progress()
            entry = progress.get(file_name, {})
            # Reset jika file berubah
            if sha256 and entry.get("sha256") != sha256:
                print(f"[progress_manager][DEBUG] SHA256 berubah untuk {file_name}, reset entry.")
                entry = {}
            if modified_time and entry.get("modified_time") != modified_time:
                print(f"[progress_manager][DEBUG] Modified time berubah untuk {file_name}, reset entry.")
                entry = {}
            old_processed = entry.get("processed", 0)
            # Update fields utama
            entry["processed"] = max(processed, old_processed)
            if sha256: entry["sha256"] = sha256
            if modified_time: entry["modified_time"] = modified_time
            if last_batch is not None: entry["last_batch"] = last_batch
            # total TIDAK diupdate manual, selalu dinamis dari meta
            # Field auto-retry/throttle
            if retry_count is not None: entry["retry_count"] = retry_count
            if last_batch_size is not None: entry["last_batch_size"] = last_batch_size
            if last_error_type is not None: entry["last_error_type"] = last_error_type
            if consecutive_success_count is not None: entry["consecutive_success_count"] = consecutive_success_count
            # Penanda apakah total baris hasil estimasi (integrasi row_estimator)
            if is_estimated is not None:
                entry["is_estimated"] = is_estimated
            progress[file_name] = entry
            print(f"[progress_manager][DEBUG] Progress entry for {file_name}: {entry}")
            self.save_progress(progress)

    def get_total_items_from_meta(self, file_name):
        """
        Ambil jumlah total record dari csvjson_gdrive_meta.json, selalu up-to-date, dinamis.
        Fallback: jika gagal, hitung jumlah baris file CSV (tanpa header).
        """
        meta_path = self.meta_file
        try:
            if not os.path.exists(meta_path):
                raise FileNotFoundError("Meta file not found")
            with open(meta_path, "r", encoding="utf-8") as f:
                meta_data = json.load(f)
            for entry in meta_data:
                fname = entry.get("saved_name") or entry.get("file") or entry.get("original_name")
                if fname == file_name:
                    return entry.get("total_items", 0)
            # Jika tidak ditemukan di meta, fallback ke file CSV
            raise ValueError("File not found in meta")
        except Exception as e:
            print(f"[progress_manager][HYBRID-FALLBACK] get_total_items_from_meta error: {e}, fallback ke file CSV.")
            # Fallback: hitung jumlah baris di file CSV (tanpa header)
            csv_path = os.path.join(self.data_dir, file_name)
            if os.path.exists(csv_path):
                try:
                    with open(csv_path, newline='', encoding='utf-8') as csvfile:
                        row_count = sum(1 for row in csvfile)
                        return max(row_count - 1, 0)  # Kurangi header
                except Exception as e2:
                    print(f"[progress_manager][HYBRID-FALLBACK] Gagal hitung baris file {file_name}: {e2}")
                    return 0
            return 0

    def get_file_progress(self, file_name):
        """
        Ambil progres file tertentu, field 'total' diambil dari meta file, fallback ke file CSV jika error.
        """
        progress = self.load_progress()
        result = progress.get(file_name, {}).copy()
        try:
            total = self.get_total_items_from_meta(file_name)
            result["total"] = total
            result["is_estimated"] = False  # Karena meta/file diambil langsung
        except Exception as e:
            print(f"[progress_manager][HYBRID-FALLBACK] get_file_progress fallback: {e}")
            result["total"] = 0
            result["is_estimated"] = True
        # PATCH: percent_processed untuk integrasi dengan smart_file_scanner.py
        processed = result.get("processed", 0)
        total_items = result.get("total", 0)
        if total_items and total_items > 0:
            result["percent_processed"] = round((processed / total_items) * 100, 2)
        else:
            result["percent_processed"] = None
        print(f"[progress_manager][DEBUG] get_file_progress for {file_name}: {result}")
        return result

    def reset_progress(self, file_name):
        """Reset progres file tertentu."""
        with self.lock:
            print(f"[progress_manager][DEBUG] reset_progress called for: {file_name}")
            progress = self.load_progress()
            if file_name in progress:
                progress[file_name] = {}
                self.save_progress(progress)
                print(f"[progress_manager][DEBUG] Progress reset for {file_name}")

    def get_all_progress(self):
        """
        Ambil seluruh progres (untuk dashboard/monitoring).
        Field 'total' untuk setiap file diambil dari meta file, fallback ke file CSV jika error.
        PATCH: percent_processed untuk integrasi dengan smart_file_scanner.py
        """
        progress = self.load_progress()
        all_result = {}
        # Ambil meta sekali, lalu merge ke setiap file
        meta_dict = {}
        meta_error = False
        try:
            if os.path.exists(self.meta_file):
                with open(self.meta_file, "r", encoding="utf-8") as f:
                    meta_data = json.load(f)
                for entry in meta_data:
                    fname = entry.get("saved_name") or entry.get("file") or entry.get("original_name")
                    if fname:
                        meta_dict[fname] = entry.get("total_items", 0)
        except Exception as e:
            print(f"[progress_manager][HYBRID-FALLBACK] get_all_progress meta read error: {e}")
            meta_error = True
        # Gabungkan progress & meta, fallback jika perlu
        file_names = set(progress.keys()) | set(meta_dict.keys())
        # Tambahkan file dari folder data jika meta error
        if meta_error:
            try:
                csv_files = [f for f in os.listdir(self.data_dir) if f.lower().endswith(".csv")]
                file_names |= set(csv_files)
            except Exception as e:
                print(f"[progress_manager][HYBRID-FALLBACK] get_all_progress failed to list data dir: {e}")
        for fname in file_names:
            entry = progress.get(fname, {}).copy()
            # Ambil total dari meta, fallback ke CSV
            if not meta_error and fname in meta_dict:
                entry["total"] = meta_dict.get(fname, 0)
                entry["is_estimated"] = False
            else:
                # Fallback: hitung jumlah baris file CSV
                csv_path = os.path.join(self.data_dir, fname)
                try:
                    if os.path.exists(csv_path):
                        with open(csv_path, newline='', encoding='utf-8') as csvfile:
                            row_count = sum(1 for row in csvfile)
                            entry["total"] = max(row_count - 1, 0)
                            entry["is_estimated"] = True
                    else:
                        entry["total"] = 0
                        entry["is_estimated"] = True
                except Exception as e2:
                    print(f"[progress_manager][HYBRID-FALLBACK] Gagal hitung baris file {fname}: {e2}")
                    entry["total"] = 0
                    entry["is_estimated"] = True
            # PATCH: percent_processed untuk integrasi dengan smart_file_scanner.py
            processed = entry.get("processed", 0)
            total_items = entry.get("total", 0)
            if total_items and total_items > 0:
                entry["percent_processed"] = round((processed / total_items) * 100, 2)
            else:
                entry["percent_processed"] = None
            all_result[fname] = entry
        print(f"[progress_manager][DEBUG] get_all_progress (hybrid): {all_result}")
        return all_result

    def remove_file_progress(self, file_name):
        """Hapus progres file tertentu (misal: file dihapus dari disk)."""
        with self.lock:
            print(f"[progress_manager][DEBUG] remove_file_progress called for: {file_name}")
            progress = self.load_progress()
            if file_name in progress:
                del progress[file_name]
                self.save_progress(progress)
                print(f"[progress_manager][DEBUG] Progress entry removed for {file_name}")

    def sync_progress_with_files(self, scan_result=None):
        """
        Sinkron progres dengan isi folder data DAN meta file (hybrid):
        - Jika meta file gagal, fallback ke file CSV di folder data.
        - Jika folder kosong, reset progres (batch 1 semua).
        - Jika ada file baru, buat progres batch 1.
        - Jika file lama hilang (tidak ada di meta ATAU tidak ada di folder data), hapus progresnya.
        - Debug: print semua file terdeteksi dan update.
        PATCH: jika scan_result (output scan_data_folder dari smart_file_scanner.py) diberikan, gunakan untuk sinkronisasi otomatis.
        """
        with self.lock:
            print("[progress_manager][DEBUG] sync_progress_with_files called")
            progress = self.load_progress()
            # PATCH: sinkronisasi dari hasil scan (lebih general dari hanya file .csv)
            if scan_result is not None:
                scanned_names = set([f['name'] for f in scan_result])
                # Tambahkan file baru ke progres
                for f in scan_result:
                    if f['name'] not in progress:
                        print(f"[progress_manager][DEBUG] File baru terdeteksi (scanner): {f['name']}, entry progress dibuat otomatis.")
                        progress[f['name']] = {
                            "processed": 0,
                            "last_batch": 0,
                            "retry_count": 0,
                            "last_batch_size": None,
                            "last_error_type": None,
                            "consecutive_success_count": 0,
                            # PATCH: simpan sha256, modified_time, size_bytes jika ada
                            "sha256": f.get("sha256", ""),
                            "modified_time": f.get("modified_time", None),
                            "size_bytes": f.get("size_bytes", None)
                        }
                    else:
                        # PATCH: update sha256, modified_time, size_bytes jika berubah
                        entry = progress[f['name']]
                        if "sha256" in f and entry.get("sha256") != f["sha256"]:
                            entry["sha256"] = f["sha256"]
                        if "modified_time" in f and entry.get("modified_time") != f["modified_time"]:
                            entry["modified_time"] = f["modified_time"]
                        if "size_bytes" in f and entry.get("size_bytes") != f["size_bytes"]:
                            entry["size_bytes"] = f["size_bytes"]
                        progress[f['name']] = entry
                # Hapus entry progres untuk file yang sudah tidak ada
                removed_names = set(progress.keys()) - scanned_names
                for fname in removed_names:
                    print(f"[progress_manager][DEBUG] File {fname} tidak ada di hasil scan, entry progress dihapus.")
                    del progress[fname]
                self.save_progress(progress)
                print("[progress_manager][DEBUG] Progress setelah sync via scanner:", progress)
                return progress
            # --- versi legacy: jika tidak diberikan scan_result, sinkronisasi hanya file .csv dan meta seperti biasa ---
            # Ambil semua file .csv valid di folder data
            files_on_disk = {
                f for f in os.listdir(self.data_dir)
                if os.path.isfile(os.path.join(self.data_dir, f)) and f.lower().endswith('.csv') and "progress" not in f and "meta" not in f
            }
            print("[progress_manager][DEBUG] files_on_disk:", files_on_disk)
            # Ambil semua file valid dari meta file (hybrid)
            meta_names = set()
            meta_error = False
            try:
                if os.path.exists(self.meta_file):
                    with open(self.meta_file, "r", encoding="utf-8") as f:
                        meta_files = json.load(f)
                    meta_names = set([f["saved_name"] for f in meta_files if "saved_name" in f])
            except Exception as e:
                meta_error = True
                print(f"[progress_manager][HYBRID-FALLBACK] sync_progress_with_files meta read error: {e}")
            # Fallback: jika meta error, gunakan semua file di folder data sebagai valid
            if meta_error:
                valid_names = files_on_disk
            else:
                valid_names = files_on_disk & meta_names
            print("[progress_manager][DEBUG] valid_names (files_on_disk & meta_names):", valid_names)

            # Reset progress if folder is empty (batch 1)
            if not valid_names:
                self.save_progress({})
                print("[progress_manager][DEBUG] Tidak ada file valid, progress direset.")
                return {}

            # Update progress: reset/add for new files, remove for missing files
            new_progress = {}
            for fname in valid_names:
                if fname not in progress:
                    print(f"[progress_manager][DEBUG] File baru terdeteksi: {fname}, entry progress dibuat otomatis.")
                    new_progress[fname] = {
                        "processed": 0,
                        "last_batch": 0,
                        "retry_count": 0,
                        "last_batch_size": None,
                        "last_error_type": None,
                        "consecutive_success_count": 0,
                    }
                else:
                    new_progress[fname] = progress[fname]
            removed_files = set(progress.keys()) - valid_names
            for fname in removed_files:
                print(f"[progress_manager][DEBUG] File {fname} tidak ada di meta/folder data, entry progress dihapus.")
            # Hanya simpan file yang valid, jadi yang di-removed_files tidak ikut tersimpan
            self.save_progress(new_progress)
            print("[progress_manager][DEBUG] Progress terbaru setelah sync:", new_progress)
            return new_progress

if __name__ == "__main__":
    # Contoh penggunaan otomatis dan dinamis (hybrid/fallback)
    pm = ProgressManager()
    print("[progress_manager] Progress snapshot:", pm.get_all_progress())
    pm.update_progress("contoh.csv", processed=100, sha256="sha256hex", modified_time="1685420000.123", last_batch=2, retry_count=1, last_batch_size=100, last_error_type="timeout", consecutive_success_count=0, is_estimated=True)
    print("[progress_manager] Progress after update:", pm.get_all_progress())
    pm.reset_progress("contoh.csv")
    print("[progress_manager] Progress after reset:", pm.get_all_progress())
    print("[progress_manager] Sync status:", pm.sync_progress_with_files())
    print("[progress_manager] Progress after meta auto sync:", pm.get_all_progress())

4. error_handler.py:

import os
import traceback
import datetime
import threading

class ErrorHandler:
    """
    ErrorHandler: Logging error, auto-retry, simpan stacktrace.
    Thread-safe dan bisa dipakai di orchestrator, batch, atau API.
    """
    def __init__(self, log_dir=None):
        try:
            if log_dir is None:
                log_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "audit_logs")
            os.makedirs(log_dir, exist_ok=True)
            self.log_dir = log_dir
            self.log_file = os.path.join(log_dir, "error.log")
            self.lock = threading.Lock()
            print(f"[error_handler][DEBUG] ErrorHandler initialized with log_dir={self.log_dir}, log_file={self.log_file}")
        except Exception as e:
            print(f"[error_handler][HYBRID-FALLBACK][ERROR] Failed to initialize ErrorHandler: {e}")
            # Fallback: minimal attributes to avoid crash.
            self.log_dir = "."
            self.log_file = "error.log"
            self.lock = threading.Lock()

    def log_error(self, err, context=None, notify_callback=None):
        """
        Log error dengan stacktrace dan context.
        Optionally, trigger notifikasi via callback jika diberikan.
        """
        now = datetime.datetime.utcnow().isoformat()
        try:
            tb_str = "".join(traceback.format_exception(type(err), err, err.__traceback__))
        except Exception as e:
            tb_str = f"[HYBRID-FALLBACK][ERROR] Failed to format exception: {e}\n"
        log_entry = {
            "timestamp": now,
            "error": str(err),
            "context": context or "",
            "traceback": tb_str
        }
        line = f"{now} | ERROR | {context or ''}\n{tb_str}\n"
        try:
            with self.lock:
                try:
                    with open(self.log_file, "a", encoding="utf-8") as f:
                        f.write(line)
                except Exception as file_err:
                    print(f"[error_handler][HYBRID-FALLBACK][ERROR] Failed to write error log: {file_err}")
        except Exception as e:
            print(f"[error_handler][HYBRID-FALLBACK][ERROR] Locking error on log_error: {e}")
        print(f"[error_handler] Error logged: {err} | Context: {context}")
        print(f"[error_handler][DEBUG] Error log entry:\n{line}")
        # Optional: trigger notification
        if notify_callback:
            try:
                notify_callback(message=line, level="error", context=context)
            except Exception as notif_err:
                print(f"[error_handler][HYBRID-FALLBACK][ERROR] Failed to notify: {notif_err}")

    def log_info(self, msg):
        """Log info ke file dan print."""
        now = datetime.datetime.utcnow().isoformat()
        line = f"{now} | INFO  | {msg}\n"
        try:
            with self.lock:
                try:
                    with open(self.log_file, "a", encoding="utf-8") as f:
                        f.write(line)
                except Exception as file_err:
                    print(f"[error_handler][HYBRID-FALLBACK][ERROR] Failed to write info log: {file_err}")
        except Exception as e:
            print(f"[error_handler][HYBRID-FALLBACK][ERROR] Locking error on log_info: {e}")
        print(f"[error_handler] {msg}")
        print(f"[error_handler][DEBUG] Info log entry:\n{line}")

    def auto_retry(self, func, max_retries=3, context=None, notify_callback=None, *args, **kwargs):
        """
        Eksekusi func dengan auto-retry jika error. Return hasil func jika sukses, None jika gagal semua.
        """
        for attempt in range(1, max_retries + 1):
            try:
                print(f"[error_handler][DEBUG] Attempt {attempt} for {func.__name__}")
                return func(*args, **kwargs)
            except Exception as e:
                self.log_error(e, context=f"{context or func.__name__} [attempt {attempt}]", notify_callback=notify_callback)
                if attempt < max_retries:
                    self.log_info(f"Retrying {func.__name__} (attempt {attempt + 1}/{max_retries})")
        return None

    def get_recent_errors(self, n=20):
        """Ambil n error terakhir dari log."""
        if not os.path.exists(self.log_file):
            print(f"[error_handler][DEBUG] No error log file found: {self.log_file}")
            return []
        try:
            with self.lock:
                try:
                    with open(self.log_file, "r", encoding="utf-8") as f:
                        lines = f.readlines()
                except Exception as file_err:
                    print(f"[error_handler][HYBRID-FALLBACK][ERROR] Failed to read error log: {file_err}")
                    return []
        except Exception as e:
            print(f"[error_handler][HYBRID-FALLBACK][ERROR] Locking error on get_recent_errors: {e}")
            return []
        error_lines = [line for line in lines if "| ERROR |" in line]
        print(f"[error_handler][DEBUG] Found {len(error_lines)} error lines in log, returning last {n}")
        return error_lines[-n:] if error_lines else []

if __name__ == "__main__":
    # Contoh penggunaan
    handler = ErrorHandler()
    try:
        1 / 0
    except Exception as e:
        handler.log_error(e, context="Test ZeroDivisionError")
    handler.log_info("Sample info log")
    print("[error_handler] Recent errors:", handler.get_recent_errors())

5. sync_progress.py:

from progress_manager import ProgressManager

DATA_DIR = r"C:\Users\ASUS\kpifinance-api\backend-python\data"

def sync_progress():
    pm = ProgressManager(data_dir=DATA_DIR)
    with pm.lock:
        pm.sync_progress_with_files()
        print("[SyncProgress] Progress synced safely")

if __name__ == "__main__":
    try:
        sync_progress()
    except Exception as e:
        print(f"[HYBRID-FALLBACK] ProgressManager sync error: {e}")
        # Fallback: No-op, progress will be synced in the next process run if needed

6. sync_meta_total_items.py:

import os
import json
from progress_manager import ProgressManager

DATA_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "data")
META_FILE = os.path.join(DATA_DIR, "csvjson_gdrive_meta.json")

def count_csv_rows(csv_path):
    try:
        with open(csv_path, 'r', encoding='utf-8') as f:
            return sum(1 for _ in f) - 1
    except Exception as e:
        print(f"[HYBRID-FALLBACK][ERROR] count_csv_rows: {e}")
        return 0

def sync_meta_total_items():
    pm = ProgressManager(DATA_DIR)
    with pm.lock:
        print("[SyncMeta] Syncing meta total items with locking")
        # Hybrid: Try to load meta file, fallback to file listing if needed
        try:
            with open(META_FILE, "r", encoding="utf-8") as f:
                files = json.load(f)
        except Exception as e:
            print(f"[HYBRID-FALLBACK] Failed to load meta file: {e}")
            # Fallback: list all .csv files in DATA_DIR
            files = []
            try:
                for fname in os.listdir(DATA_DIR):
                    if fname.lower().endswith('.csv'):
                        files.append({
                            "saved_name": fname,
                            "original_name": fname,
                            "total_items": 0
                        })
            except Exception as e2:
                print(f"[HYBRID-FALLBACK] Fallback listdir failed: {e2}")
                files = []
        for info in files:
            csv_path = os.path.join(DATA_DIR, info.get("saved_name", ""))
            info["total_items"] = count_csv_rows(csv_path)
        try:
            with open(META_FILE, "w", encoding="utf-8") as f:
                json.dump(files, f, indent=2, ensure_ascii=False)
            print("[INFO] csvjson_gdrive_meta.json updated with fresh total_items")
        except Exception as e:
            print(f"[HYBRID-FALLBACK] Failed to write meta file: {e}")
            # Fallback: print to stdout for manual recovery
            print(json.dumps(files, indent=2, ensure_ascii=False))

if __name__ == "__main__":
    sync_meta_total_items()

7. notification_manager.py:

import os
import smtplib
import threading
from email.message import EmailMessage
import datetime

class NotificationManager:
    """
    NotificationManager: Kirim notifikasi ke email (atau channel lain).
    Bisa diintegrasikan dengan error_handler, orchestrator, dsb.
    """
    def __init__(self, email_config=None):
        """
        email_config: dict, contoh:
        {
            'smtp_host': 'smtp.gmail.com',
            'smtp_port': 587,
            'smtp_user': 'your_email@gmail.com',
            'smtp_pass': 'your_app_password',
            'from_email': 'your_email@gmail.com',
            'to_email': ['recipient1@gmail.com', 'recipient2@gmail.com'],
            'use_tls': True
        }
        """
        self.email_config = email_config or {}
        self.lock = threading.Lock()
        print(f"[notification_manager][DEBUG] NotificationManager initialized with config: {self.email_config}")

    def send_email(self, subject, message, html_message=None):
        """
        Kirim email notifikasi.
        """
        cfg = self.email_config
        print(f"[notification_manager][DEBUG] send_email called with subject: {subject}")
        if not all(k in cfg for k in ['smtp_host', 'smtp_port', 'smtp_user', 'smtp_pass', 'from_email', 'to_email']):
            print("[notification_manager] Email config incomplete, cannot send email.")
            print(f"[notification_manager][DEBUG] Current config: {cfg}")
            return False
        try:
            msg = EmailMessage()
            msg['Subject'] = subject
            msg['From'] = cfg['from_email']
            msg['To'] = ", ".join(cfg['to_email']) if isinstance(cfg['to_email'], list) else cfg['to_email']
            msg.set_content(message)
            if html_message:
                msg.add_alternative(html_message, subtype='html')

            with self.lock:
                print(f"[notification_manager][DEBUG] Sending email via SMTP: {cfg['smtp_host']}:{cfg['smtp_port']}")
                try:
                    with smtplib.SMTP(cfg['smtp_host'], cfg['smtp_port']) as smtp:
                        if cfg.get('use_tls', True):
                            smtp.starttls()
                            print("[notification_manager][DEBUG] TLS started.")
                        smtp.login(cfg['smtp_user'], cfg['smtp_pass'])
                        print("[notification_manager][DEBUG] SMTP login successful.")
                        smtp.send_message(msg)
                    print("[notification_manager] Email sent.")
                    return True
                except Exception as smtp_e:
                    print(f"[notification_manager][HYBRID-FALLBACK][ERROR] SMTP send failed: {smtp_e}")
                    return False
        except Exception as e:
            print(f"[notification_manager][HYBRID-FALLBACK][ERROR] Failed to construct/send email: {e}")
            print(f"[notification_manager][DEBUG] Exception info: {e}")
            return False

    def notify(self, message, level="info", context=None):
        """
        Fungsi notifikasi umum, bisa digunakan oleh error_handler, orchestrator, dsb.
        Extend untuk slack/telegram/notif channel lain jika perlu.
        """
        subject = f"[{level.upper()}] Agentic Batch Notification"
        now = datetime.datetime.utcnow().strftime("%Y-%m-%d %H:%M:%S UTC")
        body = f"{now}\nLevel: {level}\nContext: {context or '-'}\n\n{message}"
        print(f"[notification_manager][DEBUG] notify called: subject={subject}, body={body}")
        try:
            return self.send_email(subject, body)
        except Exception as e:
            print(f"[notification_manager][HYBRID-FALLBACK][ERROR] notify failed: {e}")
            return False

if __name__ == "__main__":
    # Contoh penggunaan
    config = {
        'smtp_host': 'smtp.gmail.com',
        'smtp_port': 587,
        'smtp_user': 'your_email@gmail.com',
        'smtp_pass': 'your_app_password',
        'from_email': 'your_email@gmail.com',
        'to_email': ['recipient1@gmail.com'],
        'use_tls': True
    }
    notif = NotificationManager(email_config=config)
    notif.notify("Test notification from NotificationManager", level="info", context="UnitTest")

8. all_data_backend.py:

from fastapi import FastAPI, Request, Query, HTTPException, APIRouter, BackgroundTasks
from fastapi.responses import JSONResponse, FileResponse
from fastapi.middleware.cors import CORSMiddleware
from typing import Optional
import os
import json
import hashlib
import datetime
import subprocess

from utils_gdrive import ensure_gdrive_data
from smart_file_loader import (
    load_all_csv_json_tables,
    get_first_csv_json_file_path,
    smart_load_all_tables,
    get_first_data_file_path,
)
from batch_controller import run_batch_controller
from progress_manager import ProgressManager

# === CONFIG & INIT ===
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data")
os.makedirs(DATA_DIR, exist_ok=True)

PER_FILE_MAX = 10000
TOTAL_MAX = 15000

GDRIVE_FOLDER_ID_CSVJSON = "1nDveh8erKuSFTghCHhlPEEZmu9SUo8jB"
SERVICE_ACCOUNT_JSON_PATH_CSVJSON = os.path.join(BASE_DIR, "gdrive_service_account_csvjson.json")
GDRIVE_FOLDER_ID_OTHER = "1Yx_Zeil9xoFPxS6-QZTBpwnhmaE2pSzu"
SERVICE_ACCOUNT_JSON_PATH_OTHER = os.path.join(BASE_DIR, "gdrive_service_account.json")

# === DATA PROGRESS TRACKING (gunakan progress_manager) ===
pm = ProgressManager(DATA_DIR)

def calc_sha256_from_file(path):
    try:
        hash_sha256 = hashlib.sha256()
        with open(path, "rb") as f:
            for chunk in iter(lambda: f.read(8192), b""):
                hash_sha256.update(chunk)
        return hash_sha256.hexdigest()
    except Exception as e:
        print(f"[DEBUG] calc_sha256_from_file failed for {path}: {e}")
        return ""

app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.post("/force_sync_progress")
def force_sync_progress():
    print("[DEBUG] /force_sync_progress called (force-refresh progress from files)")
    pm.sync_progress_with_files()
    print("[DEBUG] /force_sync_progress finished")
    return {"status": "force synced"}

@app.get("/file_row_status")
def file_row_status(
    file: Optional[str] = Query(None, description="Nama file (filter)"),
    is_estimated: Optional[bool] = Query(None, description="True=estimasi, False=real count"),
):
    try:
        progress = pm.get_all_progress()
        result = []
        for fname, entry in progress.items():
            if file and fname != file:
                continue
            if is_estimated is not None and entry.get("is_estimated", True) != is_estimated:
                continue
            result.append({
                "file": fname,
                "total": entry.get("total", 0),
                "is_estimated": entry.get("is_estimated", True),
                "processed": entry.get("processed", 0)
            })
        return result
    except Exception as e:
        print(f"[file_row_status][HYBRID-FALLBACK] Error: {e}")
        result = []
        try:
            files = [f for f in os.listdir(DATA_DIR) if f.lower().endswith('.csv')]
            for fname in files:
                try:
                    with open(os.path.join(DATA_DIR, fname), newline='', encoding='utf-8') as csvfile:
                        row_count = sum(1 for row in csvfile)
                        result.append({
                            "file": fname,
                            "total": max(row_count - 1, 0),
                            "is_estimated": True,
                            "processed": 0
                        })
                except Exception as e2:
                    print(f"[file_row_status][HYBRID-FALLBACK] Failed to count {fname}: {e2}")
        except Exception as e2:
            print(f"[file_row_status][HYBRID-FALLBACK] Fallback failed: {e2}")
        return result

@app.post("/trigger_gdrive_sync")
def trigger_gdrive_sync():
    log = []
    try:
        print("[DEBUG] trigger_gdrive_sync: Syncing csvjson folder")
        ensure_gdrive_data(GDRIVE_FOLDER_ID_CSVJSON, DATA_DIR, SERVICE_ACCOUNT_JSON_PATH_CSVJSON, meta_prefix="csvjson")
        log.append("Synced csvjson folder")
    except Exception as e:
        log.append(f"Failed to sync csvjson: {e}")
        print(f"[DEBUG] trigger_gdrive_sync: Failed to sync csvjson: {e}")
    try:
        print("[DEBUG] trigger_gdrive_sync: Syncing other folder")
        ensure_gdrive_data(GDRIVE_FOLDER_ID_OTHER, DATA_DIR, SERVICE_ACCOUNT_JSON_PATH_OTHER, meta_prefix="other")
        log.append("Synced other folder")
    except Exception as e:
        log.append(f"Failed to sync other: {e}")
        print(f"[DEBUG] trigger_gdrive_sync: Failed to sync other: {e}")
    print(f"[DEBUG] trigger_gdrive_sync: log={log}")
    return JSONResponse({"status": "done", "log": log})

@app.post("/sync_progress")
def sync_progress():
    print("[DEBUG] /sync_progress called")
    pm.sync_progress_with_files()
    print("[DEBUG] /sync_progress finished")
    return {"status": "synced"}

@app.post("/sync_meta_total_items")
def sync_meta_total_items():
    print("[DEBUG] /sync_meta_total_items called")
    script_path = os.path.join(BASE_DIR, "sync_meta_total_items.py")
    if not os.path.exists(script_path):
        print(f"[ERROR] /sync_meta_total_items: {script_path} not found")
        raise HTTPException(status_code=500, detail="sync_meta_total_items.py not found")
    result = subprocess.run(["python", script_path], capture_output=True, text=True)
    print(f"[DEBUG] /sync_meta_total_items: returncode={result.returncode}, stdout={result.stdout}, stderr={result.stderr}")
    if result.returncode != 0:
        raise HTTPException(status_code=500, detail=f"Error running sync_meta_total_items.py: {result.stderr}")
    return {"status": "synced", "stdout": result.stdout}

@app.post("/run_batch_orchestrator")
def run_batch_orchestrator(background_tasks: BackgroundTasks):
    print("[DEBUG] /run_batch_orchestrator called")
    def _run():
        print("[DEBUG] /run_batch_orchestrator background main_loop start")
        from agentic_batch_orchestrator import main_loop
        main_loop()
        print("[DEBUG] /run_batch_orchestrator background main_loop finished")
    background_tasks.add_task(_run)
    return {"status": "started"}

@app.post("/sync_after_batch")
def sync_after_batch(background_tasks: BackgroundTasks):
    print("[DEBUG] /sync_after_batch called")
    def do_sync():
        import subprocess
        subprocess.run(["python", "sync_files.py"])
    background_tasks.add_task(do_sync)
    return {"status": "sync started"}

def _detect_file(tname, tdict, data_dir):
    filename = tdict.get('filename') or tdict.get('file_path') or tdict.get('saved_name') or None
    if filename and os.path.basename(filename):
        filename = os.path.basename(filename)
    else:
        candidates = []
        for f in os.listdir(data_dir):
            fname, ext = os.path.splitext(f)
            if fname == tname or f == tname or f.startswith(tname):
                candidates.append(f)
        filename = candidates[0] if candidates else tname
    print(f"[DEBUG] _detect_file: tname={tname}, detected filename={filename}")
    return filename

def collect_tabular_data(data_dir, only_table=None, include_progress=True, only_processed=True):
    from progress_manager import ProgressManager
    import threading
    pm = ProgressManager(data_dir)
    print(f"[DEBUG] collect_tabular_data: only_table={only_table}, only_processed={only_processed}")
    # PATCH: File lock pada progress untuk batch paralel
    with pm._lock:
        tables_csv = load_all_csv_json_tables(data_dir)
        print(f"[DEBUG] collect_tabular_data: loaded tables_csv={list(tables_csv.keys())}")
        tables_other = smart_load_all_tables(data_dir)
        print(f"[DEBUG] collect_tabular_data: loaded tables_other={list(tables_other.keys())}")
        file_entries = []
        keys = [only_table] if only_table else list(tables_csv.keys()) + list(tables_other.keys())
        for tname in keys:
            tdict = tables_csv.get(tname) or tables_other.get(tname)
            if not tdict:
                continue
            filename = _detect_file(tname, tdict, data_dir)
            if filename == "file_progress.json":
                print(f"[DEBUG] collect_tabular_data: skipping file_progress.json")
                continue
            data = tdict.get('data', [])
            if (
                data
                and isinstance(data, list)
                and all(isinstance(row, dict) for row in data)
                and not (
                    set(data[0].keys()) >= {'id', 'original_name', 'saved_name', 'mimeType', 'md5Checksum', 'modifiedTime'}
                    and len(data[0]) <= 6
                )
            ):
                fpath = os.path.join(data_dir, filename)
                try:
                    size_bytes = os.path.getsize(fpath)
                except Exception as e:
                    print(f"[DEBUG] collect_tabular_data: os.path.getsize failed for {fpath}: {e}")
                    size_bytes = len(json.dumps(data, default=str).encode("utf-8"))
                file_entries.append((tname, tdict, filename, size_bytes))
        file_entries = sorted(file_entries, key=lambda x: x[3])
        merged = []
        for tname, tdict, filename, _ in file_entries:
            data = tdict.get('data', [])
            file_prog = pm.get_file_progress(filename)
            processed = file_prog.get('processed', 0) if (file_prog and only_processed) else None
            print(f"[DEBUG] collect_tabular_data: file={filename}, processed={processed}, total_rows={len(data)}")
            if processed is not None and processed > 0:
                filtered_data = data[:processed]
            elif processed is not None and processed == 0:
                filtered_data = []
            else:
                filtered_data = data
            for row in filtered_data:
                row_with_file = dict(row)
                row_with_file['data_file'] = filename
                if filename == "frontend_data.json":
                    row_with_file['data_source'] = "frontend data"
                else:
                    row_with_file['data_source'] = "backend data"
                if include_progress:
                    if file_prog:
                        row_with_file['progress'] = file_prog
                merged.append(row_with_file)
        print(f"[DEBUG] collect_tabular_data: merged data length={len(merged)}")
        return merged

def list_all_tables(data_dir):
    print(f"[DEBUG] list_all_tables called")
    tables_csv = load_all_csv_json_tables(data_dir)
    tables_other = smart_load_all_tables(data_dir)
    valid_tables = []
    for tdict in list(tables_csv.values()) + list(tables_other.values()):
        data = tdict.get('data', [])
        if (
            data
            and isinstance(data, list)
            and all(isinstance(row, dict) for row in data)
            and not (
                set(data[0].keys()) >= {'id', 'original_name', 'saved_name', 'mimeType', 'md5Checksum', 'modifiedTime'}
                and len(data[0]) <= 6
            )
        ):
            valid_tables.append(tdict.get("table_name", None))
    result_tables = list(tables_csv.keys()) + list(tables_other.keys())
    print(f"[DEBUG] list_all_tables: result_tables={result_tables}")
    return result_tables

@app.get("/")
def root():
    print("[DEBUG] root called")
    return {"message": "FastAPI backend is running!"}

@app.get("/list_tables")
def api_list_tables():
    print("[DEBUG] api_list_tables called")
    return JSONResponse(content={"tables": list_all_tables(DATA_DIR)})

@app.get("/all_data_merge")
def api_all_data_merge(
    limit: int = Query(15000, ge=1, le=100000),
    offset: int = Query(0, ge=0),
    table: str = Query(None)
):
    print(f"[DEBUG] api_all_data_merge called: limit={limit}, offset={offset}, table={table}")
    try:
        merged = collect_tabular_data(DATA_DIR, only_table=table, include_progress=False, only_processed=True)
        paged_data = merged[offset:offset+limit]
        print(f"[DEBUG] api_all_data_merge: paged_data length={len(paged_data)}")
        return JSONResponse(content=paged_data)
    except Exception as e:
        print(f"[all_data_merge][HYBRID-FALLBACK] Error: {e}, fallback ke file CSV langsung")
        paged_data = []
        try:
            files = [f for f in os.listdir(DATA_DIR) if f.lower().endswith('.csv')]
            for fname in files:
                csv_path = os.path.join(DATA_DIR, fname)
                try:
                    with open(csv_path, newline='', encoding='utf-8') as csvfile:
                        import csv as csvmod
                        reader = csvmod.DictReader(csvfile)
                        for i, row in enumerate(reader):
                            file_prog = pm.get_file_progress(fname)
                            processed = file_prog.get('processed', 0) if file_prog else 0
                            if processed and i >= processed:
                                break
                            row['data_file'] = fname
                            row['data_source'] = "backend data"
                            paged_data.append(row)
                except Exception as e2:
                    print(f"[all_data_merge][HYBRID-FALLBACK] Failed to read {fname}: {e2}")
            paged_data = paged_data[offset:offset+limit]
        except Exception as e2:
            print(f"[all_data_merge][HYBRID-FALLBACK] Fallback total failure: {e2}")
        return JSONResponse(content=paged_data)

@app.post("/all_data_merge")
@app.put("/all_data_merge")
@app.patch("/all_data_merge")
async def api_all_data_merge_post(
    request: Request,
    limit: int = Query(15000, ge=1, le=100000),
    offset: int = Query(0, ge=0),
    table: str = Query(None)
):
    print(f"[DEBUG] api_all_data_merge_post called: limit={limit}, offset={offset}, table={table}")
    max_size = 100 * 1024 * 1024  # 100MB
    try:
        body = await request.body()
        if len(body) > max_size:
            print("[DEBUG] api_all_data_merge_post: body too large")
            raise HTTPException(status_code=413, detail="Payload too large (max 100 MB)")
        data = await request.json()
        print(f"[DEBUG] api_all_data_merge_post: received data type={type(data)}")
        if isinstance(data, list):
            merged = data[offset:offset+limit]
        elif isinstance(data, dict):
            if 'data' in data and isinstance(data['data'], list):
                merged = data['data'][offset:offset+limit]
            else:
                merged = [data][offset:offset+limit]
        else:
            merged = []
        if not merged:
            print("[DEBUG] api_all_data_merge_post: no data in body, fallback to local")
            raise Exception("No data in body, fallback to local")
        for row in merged:
            filename = row.get('data_file') or row.get('data_source') or "frontend_data.json"
            if 'data_file' not in row:
                row['data_file'] = filename
            if filename == "frontend_data.json":
                row['data_source'] = "frontend data"
            else:
                row['data_source'] = "backend data"
        print(f"[DEBUG] api_all_data_merge_post: merged length={len(merged)}")
        return JSONResponse(content=merged)
    except Exception as e:
        print(f"[all_data_merge_post][HYBRID-FALLBACK] Exception: {e}, fallback ke collect_tabular_data")
        try:
            merged = collect_tabular_data(DATA_DIR, only_table=table, include_progress=False, only_processed=True)
            paged_data = merged[offset:offset+limit]
            print(f"[all_data_merge_post][HYBRID-FALLBACK] paged_data length={len(paged_data)}")
            return JSONResponse(content=paged_data)
        except Exception as e2:
            print(f"[all_data_merge_post][HYBRID-FALLBACK] Fallback total failure: {e2}")
            return JSONResponse(content=[])

@app.get("/download_data")
def download_data(table: str = Query(None, description="Nama table/data yang ingin diunduh (tanpa extensi)")):
    print(f"[DEBUG] download_data called: table={table}")
    file_path, file_name, media_type = get_first_csv_json_file_path(DATA_DIR, table)
    if not file_path or not os.path.exists(file_path):
        file_path, file_name, media_type = get_first_data_file_path(DATA_DIR, table)
        if not file_path or not os.path.exists(file_path):
            print(f"[DEBUG] download_data: file not found")
            raise HTTPException(status_code=404, detail="File data tidak ditemukan")
    print(f"[DEBUG] download_data: sending file {file_path}")
    return FileResponse(file_path, media_type=media_type, filename=file_name)

from upload_frontend_data import router as upload_router
app.include_router(upload_router)

from all_data_audit import router as audit_router
app.include_router(audit_router)

if __name__ == "__main__":
    import uvicorn
    print("[DEBUG] __main__ starting uvicorn")
    uvicorn.run("all_data_backend:app", host="0.0.0.0", port=8000, reload=True)

9. all_data_audit.py:

import os
import json
import hashlib

from fastapi import APIRouter
from fastapi.responses import JSONResponse

from progress_manager import ProgressManager

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data")
PROGRESS_FILE = os.path.join(DATA_DIR, "file_progress.json")

router = APIRouter()
pm = ProgressManager(DATA_DIR)

def calc_sha256_from_file(path):
    try:
        hash_sha256 = hashlib.sha256()
        with open(path, "rb") as f:
            for chunk in iter(lambda: f.read(8192), b""):
                hash_sha256.update(chunk)
        sha = hash_sha256.hexdigest()
        print(f"[DEBUG] calc_sha256_from_file: path={path}, sha256={sha}")
        return sha
    except Exception as e:
        print(f"[DEBUG] calc_sha256_from_file: failed for path={path}, error={e}")
        return ""

def compute_status(processed_items, total_items, last_error_type):
    if total_items == 0:
        return "no_data"
    if processed_items >= total_items:
        return "finished"
    if last_error_type:
        return "error"
    if processed_items > 0:
        return "processing"
    return "pending"

@router.get("/audit_progress")
def audit_progress():
    pm = ProgressManager(DATA_DIR)
    with pm.lock:
        progress = pm.get_all_progress()
        print(f"[Audit] Progress: {progress}")
        return progress

@router.get("/all_data_audit")
def all_data_audit_get():
    print("[DEBUG] all_data_audit_get: called")
    all_files = []
    # PATCH: Audit membaca progress di dalam lock agar konsisten pada batch paralel/bertahap
    with pm.lock:
        try:
            # HYBRID: Use ProgressManager hybrid methods for robust progress/meta reading
            progress = pm.get_all_progress()
        except Exception as e:
            print(f"[all_data_audit][HYBRID-FALLBACK] ProgressManager get_all_progress error: {e}")
            # Fallback: try to read progress file directly
            progress = {}
            try:
                if os.path.exists(PROGRESS_FILE):
                    with open(PROGRESS_FILE, "r", encoding="utf-8") as f:
                        progress = json.load(f)
                        print(f"[DEBUG] load_progress fallback: {progress}")
            except Exception as e2:
                print(f"[all_data_audit][HYBRID-FALLBACK] Fallback load_progress failed: {e2}")
                progress = {}

        for meta_prefix in ["csvjson", "other"]:
            meta_path = os.path.join(DATA_DIR, f"{meta_prefix}_gdrive_meta.json")
            print(f"[DEBUG] all_data_audit_get: checking meta_path: {meta_path}")
            try:
                if os.path.exists(meta_path):
                    print(f"[DEBUG] all_data_audit_get: meta_path exists: {meta_path}")
                    with open(meta_path, "r", encoding="utf-8") as f:
                        files = json.load(f)
                        print(f"[DEBUG] all_data_audit_get: loaded {len(files)} files from {meta_path}")
                else:
                    files = []
            except Exception as e:
                print(f"[all_data_audit][HYBRID-FALLBACK] Failed to load meta {meta_path}: {e}")
                # Fallback: list csv files in DATA_DIR
                try:
                    files = []
                    for fname in os.listdir(DATA_DIR):
                        if fname.lower().endswith('.csv'):
                            files.append({
                                "saved_name": fname,
                                "original_name": fname,
                                "total_items": 0,  # Will fallback below
                                "md5Checksum": "",
                                "mimeType": "",
                                "modifiedTime": "",
                            })
                except Exception as e2:
                    print(f"[all_data_audit][HYBRID-FALLBACK] Fallback to listdir failed: {e2}")
                    files = []

            for info in files:
                fpath = os.path.join(DATA_DIR, info.get("saved_name", ""))
                print(f"[DEBUG] all_data_audit_get: processing file: {fpath}")
                try:
                    size_bytes = os.path.getsize(fpath) if os.path.exists(fpath) else 0
                except Exception as e:
                    print(f"[DEBUG] getsize failed for {fpath}: {e}")
                    size_bytes = 0
                sha256 = calc_sha256_from_file(fpath) if os.path.exists(fpath) else ""

                # HYBRID: Try to get total_items from ProgressManager/meta, fallback to count rows
                try:
                    total_items = pm.get_total_items_from_meta(info.get("saved_name", ""))
                except Exception as e:
                    print(f"[all_data_audit][HYBRID-FALLBACK] get_total_items_from_meta failed: {e}")
                    # Fallback: count rows in file
                    total_items = 0
                    try:
                        if os.path.exists(fpath):
                            with open(fpath, newline='', encoding='utf-8') as csvfile:
                                total_items = max(sum(1 for row in csvfile) - 1, 0)
                    except Exception as e2:
                        print(f"[all_data_audit][HYBRID-FALLBACK] Fallback count rows failed: {e2}")

                progress_entry = progress.get(info.get("saved_name", {}), {})
                print(f"[DEBUG] progress_entry for {info.get('saved_name')}: {progress_entry}")
                if isinstance(progress_entry, dict):
                    processed_items = progress_entry.get("processed", 0)
                    last_batch = progress_entry.get("last_batch", 0)
                    retry_count = progress_entry.get("retry_count", 0)
                    last_batch_size = progress_entry.get("last_batch_size", None)
                    last_error_type = progress_entry.get("last_error_type", None)
                    consecutive_success_count = progress_entry.get("consecutive_success_count", 0)
                else:
                    processed_items = progress_entry if isinstance(progress_entry, int) else 0
                    last_batch = 0
                    retry_count = 0
                    last_batch_size = None
                    last_error_type = None
                    consecutive_success_count = 0
                if total_items > 0:
                    processed_items = min(processed_items, total_items)
                else:
                    processed_items = 0

                percent_processed = (processed_items / total_items * 100) if total_items > 0 else 0.0

                status = compute_status(processed_items, total_items, last_error_type)

                # FLAT FORMAT (dict per row, setiap kolom sendiri)
                file_entry = {
                    "batch": last_batch,
                    "consecutive_success_count": consecutive_success_count,
                    "file": info.get("saved_name"),
                    "last_batch_size": last_batch_size,
                    "last_error_type": last_error_type,
                    "md5Checksum": info.get("md5Checksum", ""),
                    "mimeType": info.get("mimeType", ""),
                    "modified_utc": info.get("modifiedTime", ""),
                    "original_name": info.get("original_name", ""),
                    "percent_processed": round(percent_processed, 2),
                    "processed_items": processed_items,
                    "retry_count": retry_count,
                    "sha256": sha256,
                    "size_bytes": size_bytes,
                    "status": status,
                    "total_items": total_items
                }
                print(f"[DEBUG] meta_files entry: {file_entry}")
                all_files.append(file_entry)

    print(f"[DEBUG] all_data_audit_get: returning {len(all_files)} files (flat format)")
    # Output: flat list of dict, one per file (sama persis dengan backend utama)
    return JSONResponse(content=all_files)

10. batch_agent_experta.py

from experta import *
import os

class File(Fact):
    """File data untuk batch orchestration"""
    pass

class OrchestrationAgent(KnowledgeEngine):
    def __init__(self, batch_limit=15000):
        super().__init__()
        self.batch_limit = batch_limit
        self.result_plan = []
        self.used_quota = 0
        print(f"[DEBUG] OrchestrationAgent initialized with batch_limit={batch_limit}")

    @DefFacts()
    def _initial_action(self):
        print("[DEBUG] DefFacts _initial_action triggered")
        yield Fact(start=True)

    # Rule: Proses file kecil dulu, batch size = semua datanya
    @Rule(
        File(size=MATCH.size, processed=MATCH.processed, total=MATCH.total, name=MATCH.name),
        TEST(lambda size, processed, total: size <= 1000 and processed < total)
    )
    def small_file(self, size, processed, total, name):
        print(f"[DEBUG] Rule small_file triggered for {name}: size={size}, processed={processed}, total={total}")
        self.result_plan.append({'file': name, 'batch_size': 'all'})
        print(f'File kecil {name} akan diproses seluruhnya.')

    # Rule: Untuk file besar, batch dynamic sesuai sisa kuota
    @Rule(
        File(size=MATCH.size, processed=MATCH.processed, total=MATCH.total, name=MATCH.name),
        TEST(lambda size, processed, total: size > 1000 and processed < total)
    )
    def big_file(self, size, processed, total, name):
        print(f"[DEBUG] Rule big_file triggered for {name}: size={size}, processed={processed}, total={total}, used_quota={self.used_quota}")
        remaining = total - processed
        available = self.batch_limit - self.used_quota
        batch_size = min(available, remaining)
        if batch_size > 0:
            self.result_plan.append({'file': name, 'batch_size': batch_size})
            self.used_quota += batch_size
            print(f'File besar {name}, batch_size = {batch_size}')
        else:
            print(f'Kuota batch habis, skip {name}.')

    # Rule: Jika kuota batch habis, stop
    @Rule(Fact(start=True), TEST(lambda self: getattr(self, "used_quota", 0) >= getattr(self, "batch_limit", 0)))
    def quota_exceeded(self):
        print('[DEBUG] Rule quota_exceeded triggered')
        print('Kuota batch sudah habis, tidak proses file lain.')

def get_batch_plan(file_status_list, batch_limit=15000):
    print(f"[DEBUG] get_batch_plan called with batch_limit={batch_limit}")
    try:
        engine = OrchestrationAgent(batch_limit=batch_limit)
        engine.reset()
        # Prioritaskan file kecil (size <= 1000) terlebih dahulu
        sorted_list = sorted(file_status_list, key=lambda x: (x['size'], x['name']))
        print(f"[DEBUG] get_batch_plan sorted_list={sorted_list}")
        for file_info in sorted_list:
            print(f"[DEBUG] Declaring File: {file_info}")
            engine.declare(File(
                name=file_info['name'],
                size=file_info['size'],
                total=file_info['total'],
                processed=file_info['processed']
            ))
        print("[DEBUG] Running engine")
        engine.run()
        print(f"[DEBUG] get_batch_plan result_plan={engine.result_plan}")
        return engine.result_plan
    except Exception as e:
        print(f"[HYBRID-FALLBACK][ERROR] get_batch_plan failed: {e}")
        # Fallback: Plan nothing if error
        return []

11. upload_frontend_data.py

from fastapi import APIRouter, Request
import os
import json
from progress_manager import ProgressManager

router = APIRouter()

DATA_DIR = r"C:\Users\ASUS\kpifinance-api\backend-python\data"
os.makedirs(DATA_DIR, exist_ok=True)

@router.post("/upload_frontend_data")
async def upload_frontend_data(request: Request):
    print("[DEBUG] upload_frontend_data: called")
    data = await request.json()
    print(f"[DEBUG] upload_frontend_data: received data type={type(data)}, keys={list(data.keys()) if isinstance(data, dict) else 'not dict'}")
    filepath = os.path.join(DATA_DIR, "frontend_data.json")
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"[DEBUG] upload_frontend_data: data saved to {filepath}")

    # PATCH: Sinkronisasi progress/meta setelah upload data (hybrid fallback)
    pm = ProgressManager(DATA_DIR)
    try:
        pm.sync_progress_with_files()
        print("[DEBUG] upload_frontend_data: progress/meta sync selesai (hybrid)")
    except Exception as e:
        print(f"[upload_frontend_data][HYBRID-FALLBACK] sync_progress_with_files error: {e}")
        # Fallback: tidak fatal, lanjutkan saja, progress/meta akan disinkronkan otomatis di proses lain

    return {"status": "ok", "saved_to": filepath}

12. smart_file_loader.py

import os
import json
import hashlib
import importlib
from functools import lru_cache
from progress_manager import ProgressManager

# Try-imports for dependencies
def try_import(name):
    try:
        return importlib.import_module(name)
    except ImportError:
        return None

pd = try_import("pandas")
dask = try_import("dask.dataframe")
joblib = try_import("joblib")
orjson = try_import("orjson")
aiofiles = try_import("aiofiles")
chardet = try_import("chardet")
pyarrow = try_import("pyarrow")
gzip = try_import("gzip")
pdfplumber = try_import("pdfplumber")
docx = try_import("docx")
pptx = try_import("pptx")
odf = try_import("odf")
np = try_import("numpy")
camelot = try_import("camelot")
rapidfuzz = try_import("rapidfuzz")
fuzzywuzzy = try_import("fuzzywuzzy")
pydantic = try_import("pydantic")
watchdog = try_import("watchdog")

if not chardet:
    raise ImportError("chardet is required for encoding detection")

DATA_FOLDER = os.path.join(os.path.dirname(os.path.abspath(__file__)), "data")

#-----------------#
# CSV/JSON Loader #
#-----------------#
def is_csv(filename): return str(filename).strip().lower().endswith('.csv')
def is_json(filename): return str(filename).strip().lower().endswith('.json')

def detect_encoding(filepath):
    with open(filepath, 'rb') as f:
        result = chardet.detect(f.read(10000))
    return result['encoding'] or 'utf-8'

def load_csv(filepath):
    try:
        if not os.path.isfile(filepath):
            print(f"[ERROR] CSV file not found: {filepath}")
            return [], [], os.path.splitext(os.path.basename(filepath))[0]
        encoding = detect_encoding(filepath)
        if pd:
            df = pd.read_csv(filepath, encoding=encoding, dtype=str, engine='python')
            df.columns = [c.encode('utf-8').decode('utf-8-sig').strip() for c in df.columns]
            columns = list(df.columns)
            data = df.fillna('').to_dict(orient='records')
        else:
            import csv
            with open(filepath, encoding=encoding) as f:
                reader = csv.DictReader(f)
                columns = reader.fieldnames or []
                data = [row for row in reader]
        return data, columns, os.path.splitext(os.path.basename(filepath))[0]
    except Exception as e:
        print(f"[HYBRID-FALLBACK][ERROR] CSV loader failed: {filepath}: {e}")
        try:
            with open(filepath, encoding='utf-8') as f:
                header = f.readline().strip().split(',')
                data = [dict(zip(header, line.strip().split(','))) for line in f if line.strip()]
            return data, header, os.path.splitext(os.path.basename(filepath))[0]
        except Exception as e2:
            print(f"[HYBRID-FALLBACK][ERROR] CSV fallback failed: {filepath}: {e2}")
            return [], [], os.path.splitext(os.path.basename(filepath))[0]

def extract_json_records(obj):
    if isinstance(obj, list):
        if all(isinstance(item, dict) for item in obj):
            return obj
        flattened = []
        for item in obj:
            flattened.extend(extract_json_records(item))
        return flattened
    if isinstance(obj, dict) and "data" in obj and isinstance(obj["data"], list):
        return extract_json_records(obj["data"])
    if isinstance(obj, dict) and all(isinstance(v, list) for v in obj.values()) and len(obj) > 0:
        flattened = []
        for v in obj.values():
            flattened.extend(extract_json_records(v))
        return flattened
    if isinstance(obj, dict):
        return [obj]
    return []

def is_meta_file(table_name):
    lower = table_name.lower()
    if lower.endswith('_meta') or lower.endswith('gdrive_meta'):
        return True
    if lower.startswith('csvjson_gdrive_meta') or lower.startswith('other_gdrive_meta'):
        return True
    return False

def load_json(filepath):
    try:
        if not os.path.isfile(filepath):
            print(f"[ERROR] JSON file not found: {filepath}")
            return [], [], os.path.splitext(os.path.basename(filepath))[0]
        with open(filepath, 'r', encoding='utf-8') as f:
            obj = json.load(f)
            data = extract_json_records(obj)
            if not (isinstance(data, list) and all(isinstance(row, dict) for row in data)):
                return [], [], os.path.splitext(os.path.basename(filepath))[0]
        columns = []
        for row in data:
            if isinstance(row, dict):
                columns.extend(list(row.keys()))
        columns = list(dict.fromkeys(columns))
        return data, columns, os.path.splitext(os.path.basename(filepath))[0]
    except Exception as e:
        print(f"[HYBRID-FALLBACK][ERROR] JSON loader failed: {filepath}: {e}")
        try:
            data = []
            with open(filepath, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        row = json.loads(line)
                        if isinstance(row, dict):
                            data.append(row)
                    except Exception:
                        continue
            columns = []
            for row in data:
                if isinstance(row, dict):
                    columns.extend(list(row.keys()))
            columns = list(dict.fromkeys(columns))
            return data, columns, os.path.splitext(os.path.basename(filepath))[0]
        except Exception as e2:
            print(f"[HYBRID-FALLBACK][ERROR] JSON line fallback failed: {filepath}: {e2}")
            return [], [], os.path.splitext(os.path.basename(filepath))[0]

def normalize_filename(fname):
    return fname.strip().lower().replace(" ", "")

@lru_cache(maxsize=16)
def get_all_csv_json_files(data_folder=DATA_FOLDER):
    try:
        files_on_disk = os.listdir(data_folder)
    except Exception as e:
        print(f"[HYBRID-FALLBACK][ERROR] get_all_csv_json_files: {e}")
        return tuple()
    result_files = []
    for fname in files_on_disk:
        fpath = os.path.join(data_folder, fname)
        if not os.path.isfile(fpath):
            continue
        lower_fname = fname.strip().lower()
        if lower_fname.endswith('.csv') or lower_fname.endswith('.json'):
            result_files.append(fpath)
    print("[smart_file_loader] CSV/JSON files detected in folder:", [os.path.basename(f) for f in result_files])
    return tuple(result_files)

def calc_sha256_from_obj(obj):
    if orjson:
        raw = orjson.dumps(obj)
    else:
        raw = json.dumps(obj, sort_keys=True, default=str).encode("utf-8")
    return hashlib.sha256(raw).hexdigest()

def parallel_read_csv_json(files):
    def _read(f):
        if is_csv(f):
            return load_csv(f)
        elif is_json(f):
            return load_json(f)
        else:
            return [], [], os.path.basename(f)
    if joblib and len(files) > 1:
        results = joblib.Parallel(n_jobs=-1)(
            joblib.delayed(_read)(f) for f in files
        )
        return results
    else:
        return [_read(f) for f in files]

def load_all_csv_json_tables(data_folder=DATA_FOLDER):
    pm = ProgressManager(data_folder)
    with pm.lock:
        try:
            files = list(get_all_csv_json_files(data_folder))
            files_set = set(files)
            files_disk = set(
                os.path.join(data_folder, fname)
                for fname in os.listdir(data_folder)
                if os.path.isfile(os.path.join(data_folder, fname)) and (
                    fname.strip().lower().endswith('.csv') or fname.strip().lower().endswith('.json')
                )
            )
            missing_files = files_disk - files_set
            if missing_files:
                print("[smart_file_loader] New/untracked CSV/JSON files detected at runtime:", [os.path.basename(f) for f in missing_files])
                files += list(missing_files)
            results = parallel_read_csv_json(files)
        except Exception as e:
            print(f"[HYBRID-FALLBACK][ERROR] load_all_csv_json_tables: {e}")
            files = []
            results = []
        tables = {}
        for data, columns, table_name in results:
            if is_meta_file(table_name):
                continue
            if is_json(table_name + ".json") and not (isinstance(data, list) and all(isinstance(row, dict) for row in data)):
                continue
            tables[table_name] = {'columns': columns, 'data': data}
        return tables

def get_first_csv_json_file_path(data_folder=DATA_FOLDER, table_name=None):
    PRIORITY_EXTS = ['.csv', '.json']
    try:
        files = [
            f for f in os.listdir(data_folder)
            if os.path.isfile(os.path.join(data_folder, f)) and (is_csv(f) or is_json(f))
        ]
    except Exception as e:
        print(f"[HYBRID-FALLBACK][ERROR] get_first_csv_json_file_path: {e}")
        return None, None, None
    if table_name:
        norm_table = normalize_filename(table_name)
        for ext in PRIORITY_EXTS:
            for f in files:
                fname_noext, fext = os.path.splitext(f)
                if normalize_filename(fname_noext) == norm_table and fext.lower() == ext:
                    fpath = os.path.join(data_folder, f)
                    return fpath, f, get_media_type(f)
    for ext in PRIORITY_EXTS:
        for f in files:
            if f.lower().endswith(ext):
                fpath = os.path.join(data_folder, f)
                return fpath, f, get_media_type(f)
    return None, None, None

#------------------#
# Multi-Format Tab #
#------------------#
def read_any_table(filepath):
    ext = os.path.splitext(filepath)[-1].lower()
    table_name = os.path.splitext(os.path.basename(filepath))[0]
    columns = []
    data = []
    try:
        # --- IMAGE TABLES ---
        if ext in ['.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff']:
            data, columns, table_name = extract_table_from_image(filepath)
        # --- EXCEL ---
        elif ext in ['.xls', '.xlsx']:
            if pd:
                df = pd.read_excel(filepath, dtype=str, engine='openpyxl')
                df.columns = [c.strip() for c in df.columns]
                columns = list(df.columns)
                data = df.fillna('').to_dict(orient='records')
            else:
                print(f"[ERROR] pandas required for Excel file: {filepath}")
                data = []
                columns = []
        # --- PARQUET ---
        elif ext == '.parquet':
            if pd:
                df = pd.read_parquet(filepath, engine='pyarrow')
                df.columns = [c.strip() for c in df.columns]
                columns = list(df.columns)
                data = df.fillna('').to_dict(orient='records')
            else:
                print(f"[ERROR] pandas/pyarrow required for Parquet file: {filepath}")
                data = []
                columns = []
        elif ext == '.gz' and filepath.lower().endswith('.parquet.gz'):
            if pd and pyarrow and gzip:
                with gzip.open(filepath, 'rb') as f:
                    df = pd.read_parquet(f, engine='pyarrow')
                df.columns = [c.strip() for c in df.columns]
                columns = list(df.columns)
                data = df.fillna('').to_dict(orient='records')
            else:
                print(f"[ERROR] pandas/pyarrow/gzip required for Parquet GZIP file: {filepath}")
                data = []
                columns = []
        # --- PDF ---
        elif ext == '.pdf':
            if pdfplumber:
                try:
                    with pdfplumber.open(filepath) as pdf:
                        all_tables = []
                        all_columns = []
                        for page in pdf.pages:
                            tables = page.extract_tables()
                            for table in tables:
                                if table and len(table) > 1:
                                    cols = table[0]
                                    all_columns = [c.strip() if c else '' for c in cols]
                                    for row in table[1:]:
                                        all_tables.append({c: v for c, v in zip(all_columns, row)})
                        if all_tables and all_columns:
                            return all_tables, all_columns, table_name
                except Exception as e:
                    print(f"[ERROR] pdfplumber failed: {e}")
            data, columns, table_name = extract_table_camelot_pdf(filepath)
            if data and columns: return data, columns, table_name
            try:
                import tempfile
                from pdf2image import convert_from_path
                pages = convert_from_path(filepath)
                for i, page_img in enumerate(pages):
                    with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmpf:
                        page_img.save(tmpf.name)
                        data, columns, table_name = extract_table_from_image(tmpf.name)
                        if data and columns:
                            return data, columns, table_name
            except Exception as e:
                print(f"[ERROR] PDF to image failed: {e}")
            if pdfplumber:
                with pdfplumber.open(filepath) as pdf:
                    lines = []
                    for page in pdf.pages:
                        text = page.extract_text()
                        if text:
                            lines += [line.strip() for line in text.split('\n') if line.strip()]
                    data = [{'line': i, 'text': line} for i, line in enumerate(lines)]
                    columns = ['line', 'text']
                    return data, columns, table_name
        # --- DOCX ---
        elif ext == '.docx':
            if docx:
                from docx import Document
                doc = Document(filepath)
                data = []
                columns = []
                for table in doc.tables:
                    keys = [cell.text.strip() for cell in table.rows[0].cells]
                    columns = keys
                    for row in table.rows[1:]:
                        values = [cell.text.strip() for cell in row.cells]
                        data.append(dict(zip(keys, values)))
                if not data:
                    for idx, para in enumerate(doc.paragraphs):
                        t = para.text.strip()
                        if t:
                            data.append({'para_no': idx, 'text': t})
                    columns = ['para_no', 'text']
            else:
                data = []
                columns = []
        # --- PPTX ---
        elif ext == '.pptx':
            if pptx:
                from pptx import Presentation
                prs = Presentation(filepath)
                data = []
                columns = []
                for idx, slide in enumerate(prs.slides):
                    title = ''
                    for shape in slide.shapes:
                        if hasattr(shape, "text") and shape.text and not title:
                            title = shape.text.strip()
                        if hasattr(shape, "has_table") and shape.has_table:
                            tbl = shape.table
                            keys = [cell.text.strip() for cell in tbl.rows[0].cells]
                            columns = keys
                            for row in tbl.rows[1:]:
                                values = [cell.text.strip() for cell in row.cells]
                                data.append(dict(zip(keys, values)))
                    if not data:
                        slide_text = []
                        for shape in slide.shapes:
                            if hasattr(shape, "text") and shape.text:
                                slide_text.append(shape.text.strip())
                        data.append({'slide_no': idx, 'title': title, 'content': '\n'.join(slide_text)})
                if not columns:
                    columns = ['slide_no', 'title', 'content']
            else:
                data = []
                columns = []
        # --- ODT ---
        elif ext == '.odt':
            try:
                from odf.opendocument import load
                from odf.table import Table, TableRow, TableCell
                from odf.text import P
                doc = load(filepath)
                data = []
                columns = []
                tables = doc.getElementsByType(Table)
                for table in tables:
                    table_rows = table.getElementsByType(TableRow)
                    if not table_rows:
                        continue
                    header_cells = table_rows[0].getElementsByType(TableCell)
                    keys = []
                    for cell in header_cells:
                        text = "".join([str(t) for t in cell.getElementsByType(P)])
                        keys.append(text.strip())
                    columns = keys
                    for row in table_rows[1:]:
                        vals = []
                        for cell in row.getElementsByType(TableCell):
                            text = "".join([str(t) for t in cell.getElementsByType(P)])
                            vals.append(text.strip())
                        data.append(dict(zip(keys, vals)))
                if not data:
                    from odf.text import Paragraph
                    paragraphs = doc.getElementsByType(Paragraph)
                    for idx, para in enumerate(paragraphs):
                        t = str(para)
                        if t:
                            data.append({'para_no': idx, 'text': t})
                    columns = ['para_no', 'text']
            except Exception as e:
                data = []
                columns = []
        else:
            data = []
            columns = []
    except Exception as e:
        print(f"[HYBRID-FALLBACK][ERROR] read_any_table failed: {filepath}: {e}")
        data = []
        columns = []
    return data, columns, table_name

def extract_table_from_image(filepath):
    # Dummy implementation — replace with actual OCR/table extraction logic
    return [], [], os.path.splitext(os.path.basename(filepath))[0]

def extract_table_camelot_pdf(filepath):
    # Dummy implementation — replace with actual camelot logic if installed
    return [], [], os.path.splitext(os.path.basename(filepath))[0]

@lru_cache(maxsize=16)
def get_all_files(data_folder):
    try:
        return tuple(
            os.path.join(data_folder, fname)
            for fname in os.listdir(data_folder)
            if not fname.lower().endswith('.csv') and not fname.lower().endswith('.json')
            and fname.lower().endswith(('.xls', '.xlsx', '.parquet', '.parquet.gz', '.pdf', '.docx', '.pptx', '.odt', '.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff'))
        )
    except Exception as e:
        print(f"[HYBRID-FALLBACK][ERROR] get_all_files: {e}")
        return tuple()

def smart_parallel_read(files):
    if joblib and len(files) > 1:
        def _read(f):
            return read_any_table(f)
        results = joblib.Parallel(n_jobs=-1)(
            joblib.delayed(_read)(f) for f in files
        )
        return results
    else:
        return [read_any_table(f) for f in files]

def smart_dask_load(files):
    if dask and len(files) > 3:
        parquet_files = [f for f in files if f.endswith('.parquet') or f.endswith('.parquet.gz')]
        if parquet_files:
            df = dask.read_parquet(parquet_files)
        else:
            return []
        merged = df.compute()
        columns = list(merged.columns)
        data = merged.fillna('').to_dict(orient='records')
        table_name = "dask_merged"
        return [(data, columns, table_name)]
    return []

def smart_load_all_tables(data_folder, only_processed=True, pm=None):
    """
    Loader hybrid untuk semua file (CSV/JSON dan non-CSV/JSON), mengembalikan format tabel flat list of dict.
    Untuk file non-CSV/JSON, gunakan smart_file_preprocessing.py untuk integrasi preprocess-to-table.
    Jika terjadi error pada salah satu proses (misal loader native gagal), otomatis fallback ke preprocessing (hybrid).
    """
    tables = {}

    # --- CSV/JSON ---
    try:
        csv_json_tables = load_all_csv_json_tables(data_folder)
        for k, v in csv_json_tables.items():
            tables[k] = v
    except Exception as e:
        print(f"[HYBRID-FALLBACK][ERROR] smart_load_all_tables csv/json loader failed: {e}")

    # --- Non-CSV/JSON (other files) via native loader ---
    error_in_native = False
    try:
        files = list(get_all_files(data_folder))
        results = smart_parallel_read(files)
        for data, columns, table_name in results:
            if data and columns:
                tables[table_name] = {'columns': columns, 'data': data}
    except Exception as e:
        error_in_native = True
        print(f"[HYBRID-FALLBACK][ERROR] smart_load_all_tables native loader failed: {e}")

    # --- Hybrid Fallback: gunakan preprocessing jika loader native gagal atau data kosong ---
    from smart_file_preprocessing import preprocess_all_files, preprocess_to_flat_table
    try:
        preproc_result = preprocess_all_files(data_folder)
        flat_tables = preprocess_to_flat_table(preproc_result)
        for fname, v in flat_tables.items():
            # PATCH: tambahkan slicing only_processed jika pm disediakan
            if only_processed and pm is not None:
                file_prog = pm.get_file_progress(fname)
                processed = file_prog.get('processed', 0) if file_prog else None
                if processed is not None and processed > 0:
                    v['data'] = v['data'][:processed]
                elif processed is not None and processed == 0:
                    v['data'] = []
            # PATCH: Overwrite only if native loader failed OR data empty
            if fname not in tables or not tables[fname]['data']:
                tables[fname] = v
    except Exception as e:
        print(f"[HYBRID-FALLBACK][ERROR] smart_load_all_tables hybrid fallback failed: {e}")

    return tables

def get_first_data_file_path(data_folder, table_name=None):
    PRIORITY_EXTS = [
        '.parquet.gz', '.parquet', '.xlsx', '.xls',
        '.pdf', '.docx', '.pptx', '.odt', '.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff'
    ]
    try:
        files = [f for f in os.listdir(data_folder) if not f.lower().endswith('.csv') and not f.lower().endswith('.json')
                 and any(f.lower().endswith(ext) for ext in PRIORITY_EXTS)]
    except Exception as e:
        print(f"[HYBRID-FALLBACK][ERROR] get_first_data_file_path: {e}")
        return None, None, None
    if table_name:
        for ext in PRIORITY_EXTS:
            fname = table_name + ext
            fpath = os.path.join(data_folder, fname)
            if os.path.exists(fpath):
                return fpath, fname, get_media_type(fname)
    for ext in PRIORITY_EXTS:
        for f in files:
            if f.lower().endswith(ext):
                fpath = os.path.join(data_folder, f)
                return fpath, f, get_media_type(f)
    return None, None, None

def get_media_type(fname):
    fname = fname.lower()
    if fname.endswith('.csv'):
        return "text/csv"
    elif fname.endswith('.json'):
        return "application/json"
    elif fname.endswith('.parquet.gz'):
        return "application/gzip"
    elif fname.endswith('.parquet'):
        return "application/octet-stream"
    elif fname.endswith('.xlsx'):
        return "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    elif fname.endswith('.xls'):
        return "application/vnd.ms-excel"
    elif fname.endswith('.pdf'):
        return "application/pdf"
    elif fname.endswith('.docx'):
        return "application/vnd.openxmlformats-officedocument.wordprocessingml.document"
    elif fname.endswith('.pptx'):
        return "application/vnd.openxmlformats-officedocument.presentationml.presentation"
    elif fname.endswith('.odt'):
        return "application/vnd.oasis.opendocument.text"
    elif fname.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff')):
        return "image/" + fname.split('.')[-1]
    else:
        return "application/octet-stream"

# Optional: class-style interface, for extensibility in orchestrator
class SmartFileLoader:
    def __init__(self, data_folder=DATA_FOLDER, pm=None):
        self.data_folder = data_folder
        self.pm = pm

    @staticmethod
    def supported_formats():
        return [
            ".csv", ".json", ".xls", ".xlsx", ".parquet", ".parquet.gz",
            ".pdf", ".docx", ".pptx", ".odt", ".png", ".jpg", ".jpeg", ".bmp", ".tif", ".tiff"
        ]

    def load_all_csv_json_tables(self):
        return load_all_csv_json_tables(self.data_folder)

    def smart_load_all_tables(self, only_processed=True):
        # hybrid loader with fallback
        return smart_load_all_tables(self.data_folder, only_processed=only_processed, pm=self.pm)

    def get_first_csv_json_file_path(self, table_name=None):
        return get_first_csv_json_file_path(self.data_folder, table_name)

    def get_first_data_file_path(self, table_name=None):
        return get_first_data_file_path(self.data_folder, table_name)

    def calc_sha256_from_obj(self, obj):
        return calc_sha256_from_obj(obj)

    def get_media_type(self, fname):
        return get_media_type(fname)

13. smart_file_scanner.py

import os
import hashlib
import time
from progress_manager import ProgressManager

SUPPORTED_EXTS = [
    '.csv', '.json', '.xls', '.xlsx', '.parquet', '.parquet.gz', '.pdf', '.docx',
    '.pptx', '.odt', '.png', '.jpg', '.jpeg', '.bmp', '.tif', '.tiff'
]

def calc_sha256_from_file(path, block_size=65536):
    """Hitung SHA256 file, efisien untuk file besar."""
    sha256 = hashlib.sha256()
    try:
        print(f"[DEBUG] calc_sha256_from_file: {path}")
        with open(path, "rb") as f:
            for chunk in iter(lambda: f.read(block_size), b""):
                sha256.update(chunk)
        sha = sha256.hexdigest()
        print(f"[DEBUG] calc_sha256_from_file: {path} sha256={sha}")
        return sha
    except Exception as e:
        print(f"[smart_file_scanner][ERROR] calc_sha256_from_file failed for {path}: {e}")
        # Hybrid fallback: return empty string (warn but allow continue)
        return ""

def scan_data_folder(data_dir, exts=SUPPORTED_EXTS, include_hidden=False, pm=None, only_incomplete=False):
    """
    Scan folder data, deteksi semua file data valid dan formatnya.
    Return: list of dict:
        [{
            'name': 'namafile.csv',
            'path': '/full/path/namafile.csv',
            'ext': '.csv',
            'size_bytes': 12345,
            'modified_time': 1685420000.123,  # epoch
            'sha256': '...',
            'progress': {...},      # [PATCH] Tambahan: info progres jika ada (jika pm terhubung)
            'percent_processed': .. # [PATCH] Tambahan: metrik persentase progress jika ada
        }, ...]
    Sinkronisasi file: hanya proses file yang ada di folder data dan sesuai ekstensi yang didukung.
    Jika only_incomplete=True dan pm diberikan, hanya return file dengan progress belum selesai.
    """
    print(f"[DEBUG] scan_data_folder: data_dir={data_dir}, exts={exts}, include_hidden={include_hidden}")
    files = []
    try:
        files_on_disk = [
            fname for fname in os.listdir(data_dir)
            if os.path.isfile(os.path.join(data_dir, fname))
        ]
    except Exception as e:
        print(f"[smart_file_scanner][HYBRID-FALLBACK][ERROR] Failed to listdir {data_dir}: {e}")
        return []
    for fname in files_on_disk:
        if not include_hidden and fname.startswith('.'):
            print(f"[DEBUG] scan_data_folder: skip hidden {fname}")
            continue
        ext = os.path.splitext(fname)[-1].lower()
        if ext not in exts:
            print(f"[DEBUG] scan_data_folder: skip ext {fname} ({ext})")
            continue
        fpath = os.path.join(data_dir, fname)
        try:
            size_bytes = os.path.getsize(fpath)
            modified_time = os.path.getmtime(fpath)
            sha256 = calc_sha256_from_file(fpath)
            fileinfo = {
                'name': fname,
                'path': fpath,
                'ext': ext,
                'size_bytes': size_bytes,
                'modified_time': modified_time,
                'sha256': sha256
            }
            # PATCH: Tambahkan info progres dan persen processed jika pm diberikan
            progress = None
            percent_processed = None
            if pm is not None and hasattr(pm, "get_file_progress"):
                progress = pm.get_file_progress(fname)
                if progress:
                    fileinfo['progress'] = progress
                    processed = progress.get('processed', 0)
                    total = progress.get('total', None)
                    if total and total > 0:
                        percent_processed = round((processed / total) * 100, 2)
                        fileinfo['percent_processed'] = percent_processed
                    else:
                        fileinfo['percent_processed'] = None
            files.append(fileinfo)
            print(f"[DEBUG] scan_data_folder: found {fileinfo}")
        except Exception as e:
            print(f"[smart_file_scanner][HYBRID-FALLBACK][ERROR] Failed scan {fname}: {e}")
            # Hybrid fallback: continue, don't append
    # PATCH: filter only_incomplete jika diinginkan (dan pm ada)
    if only_incomplete and pm is not None:
        before_filter = len(files)
        files = [
            f for f in files
            if f.get("progress") and f["progress"].get("processed", 0) < f["progress"].get("total", 0)
        ]
        print(f"[DEBUG] scan_data_folder: only_incomplete filter: {before_filter} -> {len(files)} files")
    print(f"[DEBUG] scan_data_folder: total files found: {len(files)}")
    return files

def detect_new_and_changed_files(data_dir, prev_snapshot, pm=None, only_incomplete=False):
    """
    Bandingkan snapshot scan terbaru dengan snapshot sebelumnya (list of dict).
    Return: (list_new, list_changed, list_deleted)
    PATCH: mendukung parameter pm & only_incomplete untuk filter file-file active/incomplete.
    """
    print(f"[DEBUG] detect_new_and_changed_files: data_dir={data_dir}")
    try:
        curr_files = scan_data_folder(data_dir, pm=pm, only_incomplete=only_incomplete)
    except Exception as e:
        print(f"[smart_file_scanner][HYBRID-FALLBACK][ERROR] scan_data_folder error in detect_new_and_changed_files: {e}")
        curr_files = []
    prev_map = {f['name']: f for f in prev_snapshot}
    curr_map = {f['name']: f for f in curr_files}

    new_files = [f for f in curr_files if f['name'] not in prev_map]
    changed_files = [
        f for f in curr_files
        if f['name'] in prev_map and (
            f['sha256'] != prev_map[f['name']]['sha256'] or
            f['modified_time'] != prev_map[f['name']]['modified_time']
        )
    ]
    deleted_files = [f for f in prev_snapshot if f['name'] not in curr_map]

    print(f"[DEBUG] detect_new_and_changed_files: new_files={len(new_files)}, changed_files={len(changed_files)}, deleted_files={len(deleted_files)}")
    return new_files, changed_files, deleted_files

def snapshot_to_dict(snapshot):
    """Convert snapshot list to dict {name: fileinfo}."""
    try:
        d = {f['name']: f for f in snapshot}
        print(f"[DEBUG] snapshot_to_dict: keys={list(d.keys())}")
        return d
    except Exception as e:
        print(f"[smart_file_scanner][HYBRID-FALLBACK][ERROR] snapshot_to_dict failed: {e}")
        return {}

def scan_file(data_dir, file_name):
    pm = ProgressManager(data_dir)
    with pm.lock:
        print(f"[SmartFileScanner] Scanning {file_name} with locking")
        # Scan file and optionally update progress if necessary
        fpath = os.path.join(data_dir, file_name)
        if not os.path.isfile(fpath):
            print(f"[SmartFileScanner][ERROR] {file_name} not found in {data_dir}")
            return None
        size_bytes = os.path.getsize(fpath)
        modified_time = os.path.getmtime(fpath)
        sha256 = calc_sha256_from_file(fpath)
        result = {
            'name': file_name,
            'path': fpath,
            'ext': os.path.splitext(file_name)[-1].lower(),
            'size_bytes': size_bytes,
            'modified_time': modified_time,
            'sha256': sha256
        }
        progress = pm.get_file_progress(file_name)
        if progress:
            result['progress'] = progress
            processed = progress.get('processed', 0)
            total = progress.get('total', None)
            if total and total > 0:
                result['percent_processed'] = round((processed / total) * 100, 2)
            else:
                result['percent_processed'] = None
        print(f"[SmartFileScanner] scan_file result: {result}")
        return result

if __name__ == "__main__":
    # Contoh penggunaan
    DATA_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "data")
    try:
        scan = scan_data_folder(DATA_DIR)
        print("[smart_file_scanner] Files scanned:")
        for info in scan:
            print(info)
    except Exception as e:
        print(f"[smart_file_scanner][HYBRID-FALLBACK][ERROR] main scan failed: {e}")

14. smart_file_preprocessing.py

import os
from typing import List, Dict
from progress_manager import ProgressManager

def extract_raw_lines(filepath: str) -> List[str]:
    ext = os.path.splitext(filepath)[-1].lower()
    lines = []
    print(f"[DEBUG] extract_raw_lines: processing {filepath} (ext={ext})")
    try:
        if ext == ".pdf":
            try:
                import pdfplumber
                print(f"[DEBUG] extract_raw_lines: using pdfplumber for {filepath}")
                with pdfplumber.open(filepath) as pdf:
                    for page in pdf.pages:
                        t = page.extract_text()
                        if t: lines.extend(t.split('\n'))
            except Exception as e:
                print(f"[HYBRID-FALLBACK][ERROR] pdfplumber failed for {filepath}: {e}")
                try:
                    with open(filepath, "rb") as f:
                        raw = f.read().decode('utf-8', errors='ignore')
                        lines = raw.split('\n')
                except Exception as e2:
                    print(f"[HYBRID-FALLBACK][ERROR] PDF fallback open failed for {filepath}: {e2}")
                    lines = []
        elif ext == ".docx":
            try:
                from docx import Document
                print(f"[DEBUG] extract_raw_lines: using python-docx for {filepath}")
                doc = Document(filepath)
                lines = [p.text for p in doc.paragraphs if p.text.strip()]
            except Exception as e:
                print(f"[HYBRID-FALLBACK][ERROR] python-docx failed for {filepath}: {e}")
                try:
                    with open(filepath, encoding="utf-8", errors="ignore") as f:
                        lines = f.readlines()
                except Exception as e2:
                    print(f"[HYBRID-FALLBACK][ERROR] DOCX fallback open failed for {filepath}: {e2}")
                    lines = []
        elif ext in [".png", ".jpg", ".jpeg", ".bmp", ".tif", ".tiff"]:
            try:
                from PIL import Image
                import pytesseract
                print(f"[DEBUG] extract_raw_lines: using pytesseract for {filepath}")
                t = pytesseract.image_to_string(Image.open(filepath))
                lines = t.split('\n')
            except Exception as e:
                print(f"[HYBRID-FALLBACK][ERROR] pytesseract failed for {filepath}: {e}")
                lines = []
        else:
            print(f"[DEBUG] extract_raw_lines: using open for {filepath}")
            try:
                with open(filepath, encoding="utf-8", errors="ignore") as f:
                    lines = f.readlines()
            except Exception as e:
                print(f"[HYBRID-FALLBACK][ERROR] open failed for {filepath}: {e}")
                lines = []
        clean_lines = [l.strip() for l in lines if l and l.strip()]
        print(f"[DEBUG] extract_raw_lines: extracted {len(clean_lines)} lines from {filepath}")
        return clean_lines
    except Exception as e:
        print(f"[HYBRID-FALLBACK][ERROR] Failed to preprocess {filepath}: {e}")
        return []

def preprocess_all_files(data_folder: str) -> Dict[str, Dict]:
    """
    Returns a dict: {filename: {"raw_lines": [...], "extension": ext}}
    Only processes non-CSV/JSON files.
    Sinkronisasi/pre-filter file: hanya proses file yang ada di folder dan bukan CSV/JSON.
    """
    print(f"[DEBUG] preprocess_all_files: processing folder {data_folder}")
    data = {}
    pm = ProgressManager(data_folder)
    with pm.lock:
        try:
            files_on_disk = [
                fname for fname in os.listdir(data_folder)
                if os.path.isfile(os.path.join(data_folder, fname))
            ]
        except Exception as e:
            print(f"[HYBRID-FALLBACK][ERROR] preprocess_all_files listdir failed: {e}")
            files_on_disk = []
        for fname in files_on_disk:
            ext = os.path.splitext(fname)[-1].lower()
            if ext in [".csv", ".json"]:
                print(f"[DEBUG] preprocess_all_files: skipping {fname} (CSV/JSON)")
                continue
            fpath = os.path.join(data_folder, fname)
            print(f"[DEBUG] preprocess_all_files: extracting lines from {fname}")
            raw_lines = extract_raw_lines(fpath)
            data[fname] = {
                "raw_lines": raw_lines,
                "extension": ext
            }
            print(f"[DEBUG] preprocess_all_files: {fname} -> {len(raw_lines)} lines, ext={ext}")
        print(f"[DEBUG] preprocess_all_files: processed {len(data)} files")
    return data

def preprocess_table(data_dir, table_name):
    pm = ProgressManager(data_dir)
    with pm.lock:
        print(f"[SmartFilePreprocessing] Preprocessing {table_name} with locking")
        fpath = os.path.join(data_dir, table_name)
        ext = os.path.splitext(table_name)[-1].lower()
        if ext in [".csv", ".json"]:
            print(f"[SmartFilePreprocessing] Skipping {table_name} (CSV/JSON)")
            return {}
        lines = extract_raw_lines(fpath)
        return {"raw_lines": lines, "extension": ext}

def preprocess_to_flat_table(pre_file_result: Dict[str, Dict]) -> Dict[str, Dict]:
    print("[DEBUG] preprocess_to_flat_table called")
    result = {}
    for fname, item in pre_file_result.items():
        lines = item.get("raw_lines", [])
        ext = item.get("extension", "")
        columns = ["text"]
        data = [{"text": l} for l in lines]
        result[fname] = {
            "columns": columns,
            "data": data
        }
        print(f"[DEBUG] preprocess_to_flat_table: {fname} -> {len(data)} rows, columns={columns}")
    print(f"[DEBUG] preprocess_to_flat_table: processed {len(result)} files")
    return result

15. utils_gdrive.py

import os
import io
import json
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload
from google.oauth2 import service_account
import pandas as pd  # Opsional, untuk auto clean CSV

# Link folder sesuai instruksi
CSVJSON_SOURCE = "https://drive.google.com/drive/folders/1nDveh8erKuSFTghCHhlPEEZmu9SUo8jB"
NON_CSVJSON_SOURCE = "https://drive.google.com/drive/folders/1Yx_Zeil9xoFPxS6-QZTBpwnhmaE2pSzu"

def get_gdrive_file_list(folder_id, service_account_json_path):
    SCOPES = ['https://www.googleapis.com/auth/drive.readonly']
    try:
        creds = service_account.Credentials.from_service_account_file(
            service_account_json_path, scopes=SCOPES)
        service = build('drive', 'v3', credentials=creds)
        query = f"'{folder_id}' in parents and trashed = false"
        page_token = None
        meta_files = []
        print(f"[DEBUG] get_gdrive_file_list: folder_id={folder_id}, service_account_json_path={service_account_json_path}")
        while True:
            try:
                response = service.files().list(
                    q=query,
                    spaces='drive',
                    fields='nextPageToken, files(id, name, mimeType, md5Checksum, modifiedTime)',
                    pageToken=page_token
                ).execute()
            except Exception as e:
                print(f"[GDRIVE ERROR][HYBRID-FALLBACK] Failed to list files: {e}")
                break
            files = response.get('files', [])
            for f in files:
                meta_files.append({
                    'id': f['id'],
                    'name': f['name'],
                    'md5Checksum': f.get('md5Checksum', None),
                    'modifiedTime': f.get('modifiedTime', None),
                    'mimeType': f.get('mimeType', None),
                })
            page_token = response.get('nextPageToken', None)
            if not page_token:
                break
        print(f"[GDRIVE LIST] FOLDER {folder_id} TOTAL: {len(meta_files)} FILES")
        for file in meta_files:
            print(f" - {file['name']} ({file['id']})")
        return meta_files
    except Exception as e:
        print(f"[GDRIVE ERROR][HYBRID-FALLBACK] Failed to connect to Google Drive: {e}")
        return []

def data_source_from_name(filename):
    ext = os.path.splitext(filename)[1].lower()
    if ext in [".csv", ".json"]:
        return CSVJSON_SOURCE
    return NON_CSVJSON_SOURCE

def download_all_from_gdrive_folder(folder_id, data_dir, service_account_json_path, meta_prefix="csvjson"):
    SCOPES = ['https://www.googleapis.com/auth/drive.readonly']
    try:
        creds = service_account.Credentials.from_service_account_file(
            service_account_json_path, scopes=SCOPES)
        service = build('drive', 'v3', credentials=creds)
    except Exception as e:
        print(f"[GDRIVE ERROR][HYBRID-FALLBACK] Failed to authenticate or build service: {e}")
        return []
    os.makedirs(data_dir, exist_ok=True)
    meta_files = get_gdrive_file_list(folder_id, service_account_json_path)
    meta_path = os.path.join(data_dir, f"{meta_prefix}_gdrive_meta.json")
    meta_files_written = []

    for f in meta_files:
        file_id = f['id']
        orig_name = f['name']
        dest_path = os.path.join(data_dir, orig_name)
        try:
            print(f"[GDRIVE DOWNLOAD] Downloading {orig_name}")
            request = service.files().get_media(fileId=file_id)
            with io.FileIO(dest_path, 'wb') as fh:
                downloader = MediaIoBaseDownload(fh, request)
                done = False
                while not done:
                    status, done = downloader.next_chunk()
            print(f"[GDRIVE DOWNLOAD] Done: {orig_name}")

            # Opsional: auto bersihkan duplikasi baris CSV
            if dest_path.lower().endswith('.csv'):
                try:
                    df = pd.read_csv(dest_path)
                    before = len(df)
                    df = df.drop_duplicates()
                    after = len(df)
                    if after < before:
                        df.to_csv(dest_path, index=False)
                        print(f"[PANDAS CLEAN] Removed duplicates from {orig_name}: {before-after} rows dropped")
                except Exception as e:
                    print(f"[PANDAS ERROR][HYBRID-FALLBACK] Cannot process {orig_name} as CSV: {e}")

            meta_entry = {
                "id": file_id,
                "original_name": orig_name,
                "saved_name": orig_name,
                "md5Checksum": f.get('md5Checksum', None),
                "modifiedTime": f.get('modifiedTime', None),
                "mimeType": f.get('mimeType', None),
                "data_source": data_source_from_name(orig_name),
            }

            meta_files_written.append(meta_entry)
        except Exception as e:
            print(f"[GDRIVE ERROR][HYBRID-FALLBACK] Failed to download {orig_name} ({file_id}): {e}")
            continue

    try:
        with open(meta_path, "w", encoding="utf-8") as f:
            json.dump(meta_files_written, f, indent=2)
        print(f"[GDRIVE META] Saved meta: {meta_path} ({len(meta_files_written)} files)")
    except Exception as e:
        print(f"[GDRIVE ERROR][HYBRID-FALLBACK] Failed to write meta file: {e}")
    return [os.path.join(data_dir, f['saved_name']) for f in meta_files_written]

# REVISI: Hilangkan auto download saat import/module load/server start. 
# Pindahkan pemanggilan ensure_gdrive_data ke workflow n8n/trigger eksternal saja.
# Fungsi ensure_gdrive_data TETAP ADA, tapi hanya dipanggil manual (tidak otomatis di file ini).

def ensure_gdrive_data(folder_id, data_dir, service_account_json_path, meta_prefix="csvjson"):
    print(f"[DEBUG] ensure_gdrive_data: folder_id={folder_id}, data_dir={data_dir}, meta_prefix={meta_prefix}")
    meta_path = os.path.join(data_dir, f"{meta_prefix}_gdrive_meta.json")
    try:
        remote_files = get_gdrive_file_list(folder_id, service_account_json_path)
    except Exception as e:
        print(f"[GDRIVE ERROR][HYBRID-FALLBACK] Failed to get remote file list: {e}")
        remote_files = []
    need_download = True
    if os.path.exists(meta_path):
        try:
            with open(meta_path, "r", encoding="utf-8") as f:
                old_meta = json.load(f)
            # Change all "data_file" to "data_source" in old_meta (future proofing)
            for meta in old_meta:
                if "data_file" in meta:
                    meta["data_source"] = meta.pop("data_file")
                # Revisi: pastikan data_source sesuai aturan terbaru
                if "original_name" in meta:
                    meta["data_source"] = data_source_from_name(meta["original_name"])
            old_names = set(f["saved_name"] for f in old_meta)
            remote_names = set(f["name"] for f in remote_files)
            local_files_exist = all(
                os.path.exists(os.path.join(data_dir, f["saved_name"])) for f in old_meta
            )
            print(f"[DEBUG] ensure_gdrive_data: old_names={old_names}, remote_names={remote_names}")
            print(f"[DEBUG] ensure_gdrive_data: local_files_exist={local_files_exist}")
            if old_names == remote_names and len(old_meta) == len(remote_files) and local_files_exist:
                print(f"[GDRIVE] Skipping download for {meta_prefix}, files up-to-date.")
                need_download = False
            else:
                print(f"[GDRIVE] Redownload triggered for {meta_prefix}: meta mismatch or some files missing!")
        except Exception as e:
            print(f"[GDRIVE ERROR][HYBRID-FALLBACK] Failed to open/check meta file: {e}")
    if need_download:
        print(f"[GDRIVE] Downloading all files for {meta_prefix} (force update or file count changed, or local file missing)...")
        download_all_from_gdrive_folder(folder_id, data_dir, service_account_json_path, meta_prefix=meta_prefix)
    else:
        print(f"[GDRIVE] All files for {meta_prefix} are up-to-date.")

# Tidak ada kode auto-download/ensure_gdrive_data yang dipanggil otomatis di sini.
# Panggil ensure_gdrive_data() hanya dari workflow n8n/trigger sesuai kebutuhan.

16. csvjson_gdrive_meta.json

[]

17. file_progress.json

{}

18. other_gdrive_meta.json

[]


